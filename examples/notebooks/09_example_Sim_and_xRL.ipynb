{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efad297",
   "metadata": {
    "id": "e62e00c9"
   },
   "source": [
    "# 9. Explainable Reinforcement Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db7561",
   "metadata": {
    "id": "fb3aa803"
   },
   "source": [
    "Welcome to this tutorial on **Explainable Reinforcement Learning (XRL)**! In this guide, we will explore how to interpret and explain the decisions made by reinforcement learning agents using the SHAP (SHapley Additive exPlanations) library. Through a practical example involving a simulation in a reinforcement learning setting, we'll demonstrate how to compute and visualize feature attributions for the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a32c0",
   "metadata": {
    "id": "0d793362"
   },
   "source": [
    "**Table of Contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c2689",
   "metadata": {
    "id": "87bdf688"
   },
   "source": [
    "1. [Introduction](#1-Introduction)\n",
    "\n",
    "    1.1. [Multi-Agent Deep Reinforcement Learning with Market Splitting](#11-multi-agent-deep-reinforcement-learning)\n",
    "\n",
    "2. [Explainable AI and SHAP Values](#2-explainable-ai-and-shap-values)\n",
    "\n",
    "    2.1 [Understanding Explainable AI](#21-understanding-explainable-ai)\n",
    "\n",
    "    2.2 [Introduction to SHAP Values](#22-introduction-to-shap-values)\n",
    "\n",
    "3. [Calculating SHAP Values](#3-calculating-shap-values)\n",
    "\n",
    "    3.1. [Loading and Preparing Data](#loading-and-preparing-data)\n",
    "\n",
    "    3.2. [Creating a SHAP Explainer](#32-creating-a-shap-explainer)\n",
    "\n",
    "4. [Visualizing SHAP Values](#visualizing-shap-values)\n",
    "5. [Conclusion](#conclusion)\n",
    "6. [Additional Resources](#additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc5d28",
   "metadata": {
    "id": "5e8c7fec"
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2ef10",
   "metadata": {
    "id": "06e91420"
   },
   "source": [
    "Reinforcement Learning (RL) has achieved remarkable success in various domains, such as game playing, robotics, and autonomous systems. However, RL models, particularly those using deep neural networks, are often seen as **black boxes** due to their complex architectures and non-linear computations. This opacity makes it challenging to understand and trust the decisions made by RL agents, especially in critical applications where transparency is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fcbef6",
   "metadata": {
    "id": "47b1e7ab"
   },
   "source": [
    "**Explainable Reinforcement Learning (XRL)** aims to bridge this gap by providing insights into an agent's decision-making process. By leveraging explainability techniques, we can:\n",
    "- Interpret the actions of an RL agent.\n",
    "- Understand the influence of input features on decisions.\n",
    "- Potentially improve the model's performance, fairness, and transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced3235",
   "metadata": {
    "id": "ec0717c1"
   },
   "source": [
    "In this tutorial, we will demonstrate how to apply SHAP values to a trained actor neural network in an RL framework to explain the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f01746",
   "metadata": {
    "id": "0d59bb0a",
    "lines_to_next_cell": 0
   },
   "source": [
    "### 1.1 Running a MADRL Simulation <a name=\"MARL\"></a>\n",
    "\n",
    "In this tutorial, we will simulate RL agents using a Multi-Agent Deep Reinforcement Learning (MADRL) approach. The agents operate in a market-splitting environment where they interact and learn optimal strategies over time. Here’s a breakdown of the key components:\n",
    "\n",
    "- **Observations**: Each agent receives observations, including market forecasts, unit-specific information, and past actions.\n",
    "- **Actions**: The agents decide on bidding strategies, such as bid prices for both inflexible and flexible capacities.\n",
    "- **Rewards**: The agents are rewarded based on profits and opportunity costs, helping them learn optimal bidding strategies.\n",
    "- **Algorithm**: We utilize a multi-agent version of the TD3 (Twin Delayed Deep Deterministic Policy Gradient) algorithm, which ensures stable learning even in non-stationary environments.\n",
    "\n",
    "For a more detailed explanation of the RL configurations, refer to the [Deep Reinforcement Learning Tutorial](04_reinforcement_learning_example.ipynb).\n",
    "\n",
    "### Key Aspects of the Simulation\n",
    "\n",
    "Agents require **observations** to make informed decisions, which include:\n",
    "\n",
    "- **Residual Load Forecast**: Forecasted net demand (electricity demand minus renewable generation) over the next 24 hours.\n",
    "- **Price Forecast**: Forecasted market prices over the next 24 hours.\n",
    "- **Marginal Cost**: The current marginal cost of operating the agent's power-generating unit.\n",
    "- **Previous Output**: The agent’s dispatched capacity (energy production) from the previous time step.\n",
    "\n",
    "### Agent Actions\n",
    "\n",
    "The action space for the agents is two-dimensional and consists of:\n",
    "\n",
    "- **Bid Price for Inflexible Capacity (p_inflex)**: The price at which the agent offers its minimum power output (must-run capacity) to the market.\n",
    "- **Bid Price for Flexible Capacity (p_flex)**: The price for the additional capacity above the minimum output that the agent can flexibly adjust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b17e49",
   "metadata": {
    "id": "e62e00c9"
   },
   "source": [
    "#### 1.1.1 Install Assume and Required Packages\n",
    "\n",
    "In this section, we will install the necessary packages to run the **Assume framework** along with other dependencies.\n",
    "The process is similar to the other tutorial on Assume.\n",
    "\n",
    "The following commands will install Assume and its dependencies for reinforcement learning, along with additional libraries such as Plotly for visualization.\n",
    "Make sure to install these before running the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647079b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ee220130",
    "outputId": "ffd98b47-2b07-41cd-dfe4-ff0381571825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (5.24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from plotly) (24.1)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (7.16.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (3.1.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (5.7.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (5.10.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (24.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (2.18.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (1.3.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbconvert) (5.14.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from bleach!=5.0.0->nbconvert) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from jupyter-core>=4.7->nbconvert) (4.2.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from jupyter-core>=4.7->nbconvert) (306)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbclient>=0.5.0->nbconvert) (8.6.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbformat>=5.7->nbconvert) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from nbformat>=5.7->nbconvert) (4.23.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from beautifulsoup4->nbconvert) (2.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4.1)\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Check if 'google.colab' is available\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install 'assume-framework[learning]'\n",
    "    !git clone --depth=1 https://github.com/assume-framework/assume.git assume-repo\n",
    "    # Colab currently has issues with pyomo version 6.8.2, causing the notebook to crash\n",
    "    # Installing an older version resolves this issue. This should only be considered a temporary fix.\n",
    "    !pip install pyomo==6.8.0\n",
    "!pip install plotly\n",
    "!pip install nbconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28add86",
   "metadata": {},
   "source": [
    "Define paths to differentiate between Colab or local usage.\n",
    "If you're running this on Google Colab, the paths might differ slightly from your local environment.\n",
    "You can configure the paths accordingly based on where you're executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c91474",
   "metadata": {
    "id": "e62e00c9"
   },
   "outputs": [],
   "source": [
    "# import other necessary libraries\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# import os for file operations\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import plotly for visualization\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# import yaml for reading and writing YAML files\n",
    "import yaml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95724ea",
   "metadata": {
    "id": "636ea9ae"
   },
   "source": [
    "#### 1.1.2 Create and Load Example Files from Market Splitting Tutorial\n",
    "\n",
    "To define the RL Agent, we need to obtain the results from the **Market Zone Splitting** tutorial.\n",
    "This tutorial provides essential data that the RL agent will use for decision-making.\n",
    "\n",
    "If you are working in **Google Colab**, execute the following cells to download and run the necessary notebook automatically. \n",
    "If you are working on your **local machine**, simply open the respective tutorial notebook and execute it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85fdfe19",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 08_market_zone_coupling.ipynb to notebook\n",
      "C:\\Users\\tg3533\\AppData\\Local\\miniconda3\\envs\\assume-framework\\Lib\\site-packages\\zmq\\_future.py:724: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "[NbConvertApp] Writing 196997 bytes to output.ipynb\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    # For execution in Google Colab:\n",
    "    %cd assume-repo/examples/notebooks/\n",
    "\n",
    "# Execute the Market Zone Splitting tutorial:\n",
    "!jupyter nbconvert --to notebook --execute --ExecutePreprocessor.timeout=60 --output output.ipynb 08_market_zone_coupling.ipynb\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Return to content folder (for Colab):\n",
    "    %cd /content\n",
    "\n",
    "    # Copy inputs directory to the working folder (for Colab):\n",
    "    !cp -r assume-repo/examples/notebooks/inputs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca7eab9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "233f315b",
    "outputId": "f98da7d4-0080-4546-c642-838f722965b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input CSV files have been read from 'inputs/tutorial_08'.\n"
     ]
    }
   ],
   "source": [
    "# Define the input directory\n",
    "input_dir = os.path.join(\"inputs\", \"tutorial_08\")\n",
    "\n",
    "# Read the DataFrames from CSV files\n",
    "powerplant_units = pd.read_csv(os.path.join(input_dir, \"powerplant_units.csv\"))\n",
    "demand_df = pd.read_csv(os.path.join(input_dir, \"demand_df.csv\"))\n",
    "\n",
    "print(\"Input CSV files have been read from 'inputs/tutorial_08'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc51a8",
   "metadata": {
    "id": "6985289b"
   },
   "source": [
    "#### 1.1.3 Transform the Scenario into a Learning Example\n",
    "\n",
    "The following cells show how we can convert any pre-configured scenario in Assume into a learning example.\n",
    "\n",
    "**Define a Learning Power Plant**\n",
    "\n",
    "In this example, we place a learning nuclear power plant in the southern zone. This plant has five times the maximum power of a typical plant, which allows us to create a scenario where its actions have a noticeable impact on market prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4153fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "b205256f",
    "outputId": "b9bb887b-f534-4a50-dd5b-229be1012600"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>technology</th>\n",
       "      <th>bidding_zonal</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>emission_factor</th>\n",
       "      <th>max_power</th>\n",
       "      <th>min_power</th>\n",
       "      <th>efficiency</th>\n",
       "      <th>additional_cost</th>\n",
       "      <th>node</th>\n",
       "      <th>unit_operator</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unit 11</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>naive_eom</td>\n",
       "      <td>uranium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15</td>\n",
       "      <td>north_2</td>\n",
       "      <td>Operator North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unit 12</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>naive_eom</td>\n",
       "      <td>uranium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16</td>\n",
       "      <td>north_2</td>\n",
       "      <td>Operator North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unit 13</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>naive_eom</td>\n",
       "      <td>uranium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17</td>\n",
       "      <td>north_2</td>\n",
       "      <td>Operator North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unit 14</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>naive_eom</td>\n",
       "      <td>uranium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>18</td>\n",
       "      <td>north_2</td>\n",
       "      <td>Operator North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unit 15</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>naive_eom</td>\n",
       "      <td>uranium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19</td>\n",
       "      <td>north_2</td>\n",
       "      <td>Operator North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unit 16</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>naive_eom</td>\n",
       "      <td>uranium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>south</td>\n",
       "      <td>Operator South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unit 17</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>naive_eom</td>\n",
       "      <td>uranium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21</td>\n",
       "      <td>south</td>\n",
       "      <td>Operator South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unit 18</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>naive_eom</td>\n",
       "      <td>uranium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>22</td>\n",
       "      <td>south</td>\n",
       "      <td>Operator South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unit 19</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>naive_eom</td>\n",
       "      <td>uranium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>23</td>\n",
       "      <td>south</td>\n",
       "      <td>Operator South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unit 20</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>pp_learning</td>\n",
       "      <td>uranium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>24</td>\n",
       "      <td>south</td>\n",
       "      <td>Operator-RL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        technology bidding_zonal fuel_type  emission_factor  max_power  \\\n",
       "name                                                                     \n",
       "Unit 11    nuclear     naive_eom   uranium              0.0     1000.0   \n",
       "Unit 12    nuclear     naive_eom   uranium              0.0     1000.0   \n",
       "Unit 13    nuclear     naive_eom   uranium              0.0     1000.0   \n",
       "Unit 14    nuclear     naive_eom   uranium              0.0     1000.0   \n",
       "Unit 15    nuclear     naive_eom   uranium              0.0     1000.0   \n",
       "Unit 16    nuclear     naive_eom   uranium              0.0     1000.0   \n",
       "Unit 17    nuclear     naive_eom   uranium              0.0     1000.0   \n",
       "Unit 18    nuclear     naive_eom   uranium              0.0     1000.0   \n",
       "Unit 19    nuclear     naive_eom   uranium              0.0     1000.0   \n",
       "Unit 20    nuclear   pp_learning   uranium              0.0     5000.0   \n",
       "\n",
       "         min_power  efficiency  additional_cost     node   unit_operator  \n",
       "name                                                                      \n",
       "Unit 11        0.0         0.3               15  north_2  Operator North  \n",
       "Unit 12        0.0         0.3               16  north_2  Operator North  \n",
       "Unit 13        0.0         0.3               17  north_2  Operator North  \n",
       "Unit 14        0.0         0.3               18  north_2  Operator North  \n",
       "Unit 15        0.0         0.3               19  north_2  Operator North  \n",
       "Unit 16        0.0         0.3               20    south  Operator South  \n",
       "Unit 17        0.0         0.3               21    south  Operator South  \n",
       "Unit 18        0.0         0.3               22    south  Operator South  \n",
       "Unit 19        0.0         0.3               23    south  Operator South  \n",
       "Unit 20        0.0         0.3               24    south     Operator-RL  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create scarcity in southern Germany by limiting the number of power plants\n",
    "powerplant_units = powerplant_units[:20]\n",
    "\n",
    "# Assign the RL-controlled power plant and give it market power\n",
    "powerplant_units.loc[19, \"bidding_zonal\"] = \"pp_learning\"\n",
    "powerplant_units.loc[19, \"max_power\"] = 5000  # Set maximum power to 5000 MW\n",
    "\n",
    "# Assign a specific RL unit operator to the plant\n",
    "powerplant_units.loc[19, \"unit_operator\"] = \"Operator-RL\"\n",
    "\n",
    "# Set the 'name' column as the index\n",
    "powerplant_units.set_index(\"name\", inplace=True, drop=True)\n",
    "\n",
    "# Save the updated power plant units to a CSV file\n",
    "powerplant_units.to_csv(input_dir + \"/powerplant_units.csv\")\n",
    "\n",
    "# Show the last 10 entries\n",
    "powerplant_units.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a22a61",
   "metadata": {
    "id": "cce0e8b4"
   },
   "source": [
    "**Configure Learning Hyperparameters in YAML**\n",
    "\n",
    "The following YAML configuration contains the learning-specific hyperparameters that will guide the RL agent's training process. Below is a brief description of these hyperparameters:\n",
    "\n",
    "- **continue_learning** (`False`): \n",
    "  - Whether to continue training from a previously saved state or start fresh.\n",
    "\n",
    "- **max_bid_price** (`100`): \n",
    "  - The maximum allowable bid price for the agent, used to scale the actor's output.\n",
    "\n",
    "- **algorithm** (`\"matd3\"`): \n",
    "  - The learning algorithm to be used, in this case `MATD3` (Multi-Agent Twin Delayed Deep Deterministic Policy Gradient).\n",
    "\n",
    "- **learning_rate** (`0.001`): \n",
    "  - The rate at which the model’s parameters are updated during training.\n",
    "\n",
    "- **training_episodes** (`50`): \n",
    "  - The total number of episodes for training the agent.\n",
    "\n",
    "- **episodes_collecting_initial_experience** (`3`): \n",
    "  - Number of episodes dedicated to collecting initial experience before actual training begins, during which the agent follows a random policy.\n",
    "\n",
    "- **train_freq** (`\"4h\"`): \n",
    "  - Frequency of model training, in this case, every 4 hours.\n",
    "\n",
    "- **gradient_steps** (`4`): \n",
    "  - The number of gradient updates to perform at each training step.\n",
    "\n",
    "- **batch_size** (`256`): \n",
    "  - The size of the mini-batch used for training.\n",
    "\n",
    "- **gamma** (`0.99`): \n",
    "  - The discount factor for future rewards, balancing short-term vs. long-term reward importance.\n",
    "\n",
    "- **device** (`\"cpu\"`): \n",
    "  - The computational device for training. In this case, the CPU is used.\n",
    "\n",
    "- **noise_sigma** (`0.1`): \n",
    "  - The standard deviation of the exploration noise added to actions.\n",
    "\n",
    "- **noise_scale** (`1`) and **noise_dt** (`1`): \n",
    "  - Parameters controlling the scale and time step of the exploration noise. Since both are set to 1, no decay is applied.\n",
    "\n",
    "- **validation_episodes_interval** (`3`): \n",
    "  - The interval (in episodes) at which validation is performed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c64dc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c555ce9",
    "outputId": "473126ae-3c3e-4698-e3a5-347cc00e5108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration YAML file has been saved to 'inputs\\tutorial_08\\config.yaml'.\n"
     ]
    }
   ],
   "source": [
    "# YAML configuration for the RL training\n",
    "config = {\n",
    "    \"zonal_case\": {\n",
    "        \"start_date\": \"2019-01-01 00:00\",\n",
    "        \"end_date\": \"2019-01-01 23:00\",\n",
    "        \"time_step\": \"1h\",\n",
    "        \"save_frequency_hours\": 4,\n",
    "        \"learning_mode\": \"True\",\n",
    "        \"markets_config\": {\n",
    "            \"zonal\": {\n",
    "                \"operator\": \"EOM_operator\",\n",
    "                \"product_type\": \"energy\",\n",
    "                \"products\": [{\"duration\": \"1h\", \"count\": 1, \"first_delivery\": \"1h\"}],\n",
    "                \"opening_frequency\": \"1h\",\n",
    "                \"opening_duration\": \"1h\",\n",
    "                \"volume_unit\": \"MWh\",\n",
    "                \"maximum_bid_volume\": 100000,\n",
    "                \"maximum_bid_price\": 3000,\n",
    "                \"minimum_bid_price\": -500,\n",
    "                \"price_unit\": \"EUR/MWh\",\n",
    "                \"market_mechanism\": \"complex_clearing\",\n",
    "                \"additional_fields\": [\"bid_type\", \"node\"],\n",
    "                \"param_dict\": {\"network_path\": \".\", \"zones_identifier\": \"zone_id\"},\n",
    "            }\n",
    "        },\n",
    "        \"learning_config\": {\n",
    "            \"continue_learning\": False,\n",
    "            \"max_bid_price\": 100,\n",
    "            \"algorithm\": \"matd3\",\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"training_episodes\": 15,\n",
    "            \"episodes_collecting_initial_experience\": 3,\n",
    "            \"train_freq\": \"4h\",\n",
    "            \"gradient_steps\": 4,\n",
    "            \"batch_size\": 256,\n",
    "            \"gamma\": 0.99,\n",
    "            \"device\": \"cpu\",\n",
    "            \"noise_sigma\": 0.1,\n",
    "            \"noise_scale\": 1,\n",
    "            \"noise_dt\": 1,\n",
    "            \"validation_episodes_interval\": 3,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the path for the configuration file\n",
    "config_path = os.path.join(input_dir, \"config.yaml\")\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config, file, sort_keys=False)\n",
    "\n",
    "print(f\"Configuration YAML file has been saved to '{config_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cbdb4",
   "metadata": {
    "id": "3f0f38fb"
   },
   "source": [
    "In order to make this setup compatible with XRL, we need to enhance the logging of the learning process. \n",
    "ASSUME does not have this feature natively, so we will override some functions to enable this logging for the purpose of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01977d5",
   "metadata": {
    "cellView": "form",
    "id": "201251c6"
   },
   "outputs": [],
   "source": [
    "# @title Overwrite run_learning function with enhanced logging\n",
    "# import required ASSUME modules\n",
    "from assume.common.exceptions import AssumeException\n",
    "from assume.scenario.loader_csv import setup_world\n",
    "from assume.world import World\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def run_learning(\n",
    "    world: World,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train Deep Reinforcement Learning (DRL) agents to act in a simulated market environment.\n",
    "\n",
    "    This function runs multiple episodes of simulation to train DRL agents, performs evaluation, and saves the best runs. It maintains the buffer and learned agents in memory to avoid resetting them with each new run.\n",
    "\n",
    "    Args:\n",
    "        world (World): An instance of the World class representing the simulation environment.\n",
    "        inputs_path (str): The path to the folder containing input files necessary for the simulation.\n",
    "        scenario (str): The name of the scenario for the simulation.\n",
    "        study_case (str): The specific study case for the simulation.\n",
    "\n",
    "    Note:\n",
    "        - The function uses a ReplayBuffer to store experiences for training the DRL agents.\n",
    "        - It iterates through training episodes, updating the agents and evaluating their performance at regular intervals.\n",
    "        - Initial exploration is active at the beginning and is disabled after a certain number of episodes to improve the performance of DRL algorithms.\n",
    "        - Upon completion of training, the function performs an evaluation run using the best policy learned during training.\n",
    "        - The best policies are chosen based on the average reward obtained during the evaluation runs, and they are saved for future use.\n",
    "    \"\"\"\n",
    "    from assume.reinforcement_learning.buffer import ReplayBuffer\n",
    "\n",
    "    if not verbose:\n",
    "        logger.setLevel(logging.WARNING)\n",
    "\n",
    "    # remove csv path so that nothing is written while learning\n",
    "    temp_csv_path = world.export_csv_path\n",
    "    world.export_csv_path = \"\"\n",
    "\n",
    "    # initialize policies already here to set the obs_dim and act_dim in the learning role\n",
    "    world.learning_role.rl_algorithm.initialize_policy()\n",
    "\n",
    "    # check if we already stored policies for this simulation\n",
    "    save_path = world.learning_config[\"trained_policies_save_path\"]\n",
    "\n",
    "    if Path(save_path).is_dir():\n",
    "        if world.learning_config.get(\"continue_learning\", False):\n",
    "            logger.warning(\n",
    "                f\"Save path '{save_path}' exists.\\n\"\n",
    "                \"You are in continue learning mode. New strategies may overwrite previous ones.\\n\"\n",
    "                \"It is recommended to use a different save path to avoid unintended overwrites.\\n\"\n",
    "                \"You can set 'trained_policies_save_path' in the config.\"\n",
    "            )\n",
    "            proceed = input(\n",
    "                \"Do you still want to proceed with the existing save path? (y/N) \"\n",
    "            )\n",
    "            if not proceed.lower().startswith(\"y\"):\n",
    "                raise AssumeException(\n",
    "                    \"Simulation aborted by user to avoid overwriting previous learned strategies. \"\n",
    "                    \"Consider setting a new 'simulation_id' or 'trained_policies_save_path' in the config.\"\n",
    "                )\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"Save path '{save_path}' exists. Previous training data will be deleted to start fresh.\"\n",
    "            )\n",
    "            accept = input(\"Do you want to overwrite and start fresh? (y/N) \")\n",
    "            if accept.lower().startswith(\"y\"):\n",
    "                shutil.rmtree(save_path, ignore_errors=True)\n",
    "                logger.info(\n",
    "                    f\"Previous strategies at '{save_path}' deleted. Starting fresh training.\"\n",
    "                )\n",
    "            else:\n",
    "                raise AssumeException(\n",
    "                    \"Simulation aborted by user not to overwrite existing learned strategies. \"\n",
    "                    \"You can set a different 'simulation_id' or 'trained_policies_save_path' in the config.\"\n",
    "                )\n",
    "\n",
    "    # also remove tensorboard logs\n",
    "    tensorboard_path = f\"tensorboard/{world.scenario_data['simulation_id']}\"\n",
    "    if os.path.exists(tensorboard_path):\n",
    "        shutil.rmtree(tensorboard_path, ignore_errors=True)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Information that needs to be stored across episodes, aka one simulation run\n",
    "    inter_episodic_data = {\n",
    "        \"buffer\": ReplayBuffer(\n",
    "            buffer_size=int(world.learning_config.get(\"replay_buffer_size\", 5e5)),\n",
    "            obs_dim=world.learning_role.rl_algorithm.obs_dim,\n",
    "            act_dim=world.learning_role.rl_algorithm.act_dim,\n",
    "            n_rl_units=len(world.learning_role.rl_strats),\n",
    "            device=world.learning_role.device,\n",
    "            float_type=world.learning_role.float_type,\n",
    "        ),\n",
    "        \"actors_and_critics\": None,\n",
    "        \"max_eval\": defaultdict(lambda: -1e9),\n",
    "        \"all_eval\": defaultdict(list),\n",
    "        \"avg_all_eval\": [],\n",
    "        \"episodes_done\": 0,\n",
    "        \"eval_episodes_done\": 0,\n",
    "    }\n",
    "\n",
    "    world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "    # -----------------------------------------\n",
    "\n",
    "    validation_interval = min(\n",
    "        world.learning_role.training_episodes,\n",
    "        world.learning_config.get(\"validation_episodes_interval\", 5),\n",
    "    )\n",
    "\n",
    "    # Ensure training episodes exceed the sum of initial experience and one evaluation interval\n",
    "    min_required_episodes = (\n",
    "        world.learning_role.episodes_collecting_initial_experience + validation_interval\n",
    "    )\n",
    "\n",
    "    if world.learning_role.training_episodes < min_required_episodes:\n",
    "        raise ValueError(\n",
    "            f\"Training episodes ({world.learning_role.training_episodes}) must be greater than the sum of initial experience episodes ({world.learning_role.episodes_collecting_initial_experience}) and evaluation interval ({validation_interval}).\"\n",
    "        )\n",
    "\n",
    "    eval_episode = 1\n",
    "\n",
    "    for episode in tqdm(\n",
    "        range(1, world.learning_role.training_episodes + 1),\n",
    "        desc=\"Training Episodes\",\n",
    "    ):\n",
    "        # -----------------------------------------\n",
    "        # Give the newly initialized learning role the needed information across episodes\n",
    "        if episode != 1:\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                episode=episode,\n",
    "            )\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "        world.run()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Store updated information across episodes\n",
    "        inter_episodic_data = world.learning_role.get_inter_episodic_data()\n",
    "        inter_episodic_data[\"episodes_done\"] = episode\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Store the entire buffer for xAI workflow\n",
    "        if episode == world.learning_role.training_episodes:\n",
    "            export = inter_episodic_data[\"buffer\"].observations.tolist()\n",
    "\n",
    "            with open(\n",
    "                os.path.join(\n",
    "                    world.learning_role.trained_policies_save_path, \"buffer_obs.json\"\n",
    "                ),\n",
    "                \"w\",\n",
    "            ) as f:\n",
    "                json.dump(export, f)\n",
    "\n",
    "        # evaluation run:\n",
    "        if (\n",
    "            episode % validation_interval == 0\n",
    "            and episode\n",
    "            >= world.learning_role.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.reset()\n",
    "\n",
    "            # load evaluation run\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                evaluation_mode=True,\n",
    "                eval_episode=eval_episode,\n",
    "            )\n",
    "\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "            world.run()\n",
    "\n",
    "            world.learning_role.tensor_board_logger.update_tensorboard()\n",
    "\n",
    "            total_rewards = world.output_role.get_sum_reward(eval_episode)\n",
    "\n",
    "            if len(total_rewards) == 0:\n",
    "                raise AssumeException(\"No rewards were collected during evaluation run\")\n",
    "\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "\n",
    "            # check reward improvement in evaluation run\n",
    "            # and store best run in eval folder\n",
    "            terminate = world.learning_role.compare_and_save_policies(\n",
    "                {\"avg_reward\": avg_reward}\n",
    "            )\n",
    "\n",
    "            inter_episodic_data[\"eval_episodes_done\"] = eval_episode\n",
    "\n",
    "            # if we have not improved in the last x evaluations, we stop loop\n",
    "            if terminate:\n",
    "                break\n",
    "\n",
    "            eval_episode += 1\n",
    "\n",
    "        world.reset()\n",
    "\n",
    "        # save the policies after each episode in case the simulation is stopped or crashes\n",
    "        if (\n",
    "            episode\n",
    "            >= world.learning_role.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.learning_role.rl_algorithm.save_params(\n",
    "                directory=f\"{world.learning_role.trained_policies_save_path}/last_policies\"\n",
    "            )\n",
    "\n",
    "    # container shutdown implicitly with new initialisation\n",
    "    logger.info(\"################\")\n",
    "    logger.info(\"Training finished, Start evaluation run\")\n",
    "    world.export_csv_path = temp_csv_path\n",
    "\n",
    "    world.reset()\n",
    "\n",
    "    # Set 'trained_policies_load_path' to None in order to load the most recent policies,\n",
    "    # especially if previous strategies were loaded from an external source.\n",
    "    # This is useful when continuing from a previous learning session.\n",
    "    world.scenario_data[\"config\"][\"learning_config\"][\"trained_policies_load_path\"] = (\n",
    "        f\"{world.learning_role.trained_policies_save_path}/avg_reward_eval_policies\"\n",
    "    )\n",
    "\n",
    "    # load scenario for evaluation\n",
    "    setup_world(\n",
    "        world=world,\n",
    "        terminate_learning=True,\n",
    "    )\n",
    "\n",
    "    world.learning_role.load_inter_episodic_data(inter_episodic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa6b82",
   "metadata": {
    "id": "dcacfe26"
   },
   "source": [
    "**Run the Example Case**\n",
    "\n",
    "Now we run the example case as done previously in the market zone tutorial. \n",
    "The main difference here is that we call the `run_learning()` function, which iterates multiple times over the simulation horizon for reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1c9334",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bfadf522",
    "outputId": "7c91ab13-a3c2-4e89-d8ac-d20be95391f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.world:Connected to the database\n",
      "INFO:assume.scenario.loader_csv:Input files path: inputs/tutorial_08\n",
      "INFO:assume.scenario.loader_csv:Study case: zonal_case\n",
      "INFO:assume.scenario.loader_csv:Simulation ID: tutorial_08_zonal_case\n",
      "INFO:assume.scenario.loader_csv:storage_units not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:exchange_units not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:industrial_dsm_units not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:residential_dsm_units not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:forecasts_df not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:exchanges_df not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:availability_df not found. Returning None\n",
      "WARNING:assume.scenario.loader_csv:Simulation length (0 days 23:00:00) is not divisible by train_freq (4h). This will lead to a loss of training experience.Adjusting train_freq to 3h. Consider modifying simulation length or train_freq in the config to avoid this adjustment.\n",
      "INFO:assume.scenario.loader_csv:save_frequency_hours is disabled due to CSV export being enabled. Data will be stored in the CSV files at the end of the simulation.\n",
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 1 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:00<00:00, 148368.57it/s]\n",
      "Training Episodes:   7%|▋         | 1/15 [00:00<00:09,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 2 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:00<00:00, 132057.52it/s]\n",
      "Training Episodes:  13%|█▎        | 2/15 [00:01<00:08,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 3 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:00<00:00, 128786.61it/s]\n",
      "Training Episodes:  20%|██        | 3/15 [00:02<00:08,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 4 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 53477.32it/s]\n",
      "Training Episodes:  27%|██▋       | 4/15 [00:03<00:11,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 5 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 56225.02it/s]\n",
      "Training Episodes:  33%|███▎      | 5/15 [00:05<00:12,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 6 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 53649.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Episode 1 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:00<00:00, 134310.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.reinforcement_learning.learning_role:New best policy saved, episode: 1, metric='avg_reward', value=38.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Episodes:  40%|████      | 6/15 [00:07<00:14,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 7 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 53423.63it/s]\n",
      "Training Episodes:  47%|████▋     | 7/15 [00:09<00:12,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 8 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 46964.42it/s]\n",
      "Training Episodes:  53%|█████▎    | 8/15 [00:11<00:11,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 9 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 51476.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Episode 2 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:00<00:00, 132662.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.reinforcement_learning.learning_role:New best policy saved, episode: 2, metric='avg_reward', value=38.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Episodes:  60%|██████    | 9/15 [00:13<00:11,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 10 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 52673.92it/s]\n",
      "Training Episodes:  67%|██████▋   | 10/15 [00:15<00:09,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 11 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 54487.94it/s]\n",
      "Training Episodes:  73%|███████▎  | 11/15 [00:16<00:07,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 12 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 53031.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Episode 3 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:00<00:00, 113499.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.reinforcement_learning.learning_role:New best policy saved, episode: 3, metric='avg_reward', value=38.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Episodes:  80%|████████  | 12/15 [00:19<00:05,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 13 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 45697.50it/s]\n",
      "Training Episodes:  87%|████████▋ | 13/15 [00:21<00:03,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 14 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 48294.74it/s]\n",
      "Training Episodes:  93%|█████████▎| 14/15 [00:22<00:01,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episode 15 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:01<00:00, 50022.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Episode 4 2019-01-01 22:00:00:  96%|█████████▌| 79201.0/82800 [00:00<00:00, 120091.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.reinforcement_learning.learning_role:New best policy saved, episode: 4, metric='avg_reward', value=38.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Episodes: 100%|██████████| 15/15 [00:42<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:save_frequency_hours is disabled due to CSV export being enabled. Data will be stored in the CSV files at the end of the simulation.\n",
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'load_inter_episodic_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# If learning mode is enabled, run the reinforcement learning loop\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world\u001b[38;5;241m.\u001b[39mlearning_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mrun_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Run the simulation\u001b[39;00m\n\u001b[0;32m     40\u001b[0m world\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[1;32mIn[7], line 187\u001b[0m, in \u001b[0;36mrun_learning\u001b[1;34m(world, verbose)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# load scenario for evaluation\u001b[39;00m\n\u001b[0;32m    182\u001b[0m setup_world(\n\u001b[0;32m    183\u001b[0m     world\u001b[38;5;241m=\u001b[39mworld,\n\u001b[0;32m    184\u001b[0m     terminate_learning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    185\u001b[0m )\n\u001b[1;32m--> 187\u001b[0m \u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_role\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_inter_episodic_data\u001b[49m(inter_episodic_data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'load_inter_episodic_data'"
     ]
    }
   ],
   "source": [
    "# Import necessary classes and functions from the Assume framework\n",
    "from assume.scenario.loader_csv import load_scenario_folder\n",
    "\n",
    "# Define paths for input and output data\n",
    "csv_path = \"outputs\"\n",
    "\n",
    "# Define the data format and database URI for storing results\n",
    "# Use \"local_db\" for SQLite or \"timescale\" for TimescaleDB\n",
    "os.makedirs(csv_path, exist_ok=True)\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "data_format = \"local_db\"  # Options: \"local_db\" (SQLite) or \"timescale\" (TimescaleDB)\n",
    "\n",
    "# Set the database URI based on the selected data format\n",
    "if data_format == \"local_db\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"  # SQLite database\n",
    "elif data_format == \"timescale\":\n",
    "    db_uri = \"postgresql://assume:assume@localhost:5432/assume\"  # TimescaleDB\n",
    "\n",
    "# Create the World instance with the specified database\n",
    "world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "# Load the scenario configuration\n",
    "# - world: World instance\n",
    "# - inputs_path: Folder containing input data\n",
    "# - scenario: Scenario subfolder in inputs\n",
    "# - study_case: Which configuration (case) to use for the simulation\n",
    "load_scenario_folder(\n",
    "    world,\n",
    "    inputs_path=\"inputs\",\n",
    "    scenario=\"tutorial_08\",\n",
    "    study_case=\"zonal_case\",\n",
    ")\n",
    "\n",
    "# If learning mode is enabled, run the reinforcement learning loop\n",
    "if world.learning_config.get(\"learning_mode\", False):\n",
    "    run_learning(world)\n",
    "\n",
    "# Run the simulation\n",
    "world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec3968",
   "metadata": {
    "id": "2194f71b"
   },
   "source": [
    "**Compare the Results**\n",
    "\n",
    "Next, we use the same code from the market zone tutorial to generate a Plotly graph displaying market clearing prices over time for each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762acdfe",
   "metadata": {
    "id": "bdb21cbe",
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outputs/tutorial_08_zonal_case\\\\market_meta.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m market_meta_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarket_meta.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the market metadata from the CSV file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m market_meta \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarket_meta_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m market_meta \u001b[38;5;241m=\u001b[39m market_meta\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m      8\u001b[0m     columns\u001b[38;5;241m=\u001b[39mmarket_meta\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m )  \u001b[38;5;66;03m# Drop the first unnamed column\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Extract unique zones from the \"node\" column\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tg3533\\AppData\\Local\\miniconda3\\envs\\assume-framework\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tg3533\\AppData\\Local\\miniconda3\\envs\\assume-framework\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\tg3533\\AppData\\Local\\miniconda3\\envs\\assume-framework\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tg3533\\AppData\\Local\\miniconda3\\envs\\assume-framework\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\tg3533\\AppData\\Local\\miniconda3\\envs\\assume-framework\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outputs/tutorial_08_zonal_case\\\\market_meta.csv'"
     ]
    }
   ],
   "source": [
    "# Define the path to the simulation output directory\n",
    "output_dir = \"outputs/tutorial_08_zonal_case\"\n",
    "market_meta_path = os.path.join(output_dir, \"market_meta.csv\")\n",
    "\n",
    "# Load the market metadata from the CSV file\n",
    "market_meta = pd.read_csv(market_meta_path, index_col=\"time\", parse_dates=True)\n",
    "market_meta = market_meta.drop(\n",
    "    columns=market_meta.columns[0]\n",
    ")  # Drop the first unnamed column\n",
    "\n",
    "# Extract unique zones from the \"node\" column\n",
    "zones = market_meta[\"node\"].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store clearing prices for each zone\n",
    "clearing_prices_df = pd.DataFrame()\n",
    "\n",
    "# Populate the DataFrame with clearing prices for each zone\n",
    "for zone in zones:\n",
    "    zone_data = market_meta[market_meta[\"node\"] == zone][[\"price\"]]\n",
    "    zone_data = zone_data.rename(columns={\"price\": f\"{zone}_price\"})\n",
    "    clearing_prices_df = (\n",
    "        pd.merge(\n",
    "            clearing_prices_df,\n",
    "            zone_data,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"outer\",\n",
    "        )\n",
    "        if not clearing_prices_df.empty\n",
    "        else zone_data\n",
    "    )\n",
    "\n",
    "# Sort the DataFrame by time\n",
    "clearing_prices_df = clearing_prices_df.sort_index()\n",
    "\n",
    "# Initialize the Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot clearing prices for each zone\n",
    "for zone in zones:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=clearing_prices_df.index,\n",
    "            y=clearing_prices_df[f\"{zone}_price\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"{zone} - Simulation\",\n",
    "            line=dict(width=2),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Customize the layout for better aesthetics and interaction\n",
    "fig.update_layout(\n",
    "    title=\"Clearing Prices per Zone Over Time: Simulation Results\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Clearing Price (EUR/MWh)\",\n",
    "    legend_title=\"Market Zones\",\n",
    "    xaxis=dict(\n",
    "        tickangle=45,  # Rotate x-axis labels for readability\n",
    "        type=\"date\",  # Ensure x-axis is treated as dates\n",
    "    ),\n",
    "    hovermode=\"x unified\",  # Unified hover to compare values across zones at the same time\n",
    "    template=\"plotly_white\",  # Use a clean white background\n",
    "    width=1000,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "# Display the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed563ab7",
   "metadata": {
    "id": "e77c41cf"
   },
   "source": [
    "## 2. Explainable AI and SHAP Values <a name=\"explainable-ai-and-shap-values\"></a>\n",
    "\n",
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee9432",
   "metadata": {
    "id": "cbf18570"
   },
   "source": [
    "To follow along with this tutorial, we need some additional libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a11a6",
   "metadata": {
    "id": "a5ac592c"
   },
   "source": [
    "- `matplotlib`\n",
    "- `shap`\n",
    "- `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e097c0",
   "metadata": {
    "id": "ae266ecb",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install shap==0.47.1\n",
    "!pip install scikit-learn==1.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3463ee2",
   "metadata": {
    "id": "5069ff93"
   },
   "source": [
    "### 2.1 Understanding Explainable AI\n",
    "Explainable AI (XAI) refers to techniques and methods that make the behavior and decisions of AI systems understandable to humans. In the context of complex models like deep neural networks, XAI helps to:\n",
    "- Increase Transparency: Providing insights into how models make decisions.\n",
    "- Build Trust: Users and stakeholders can trust AI systems if they understand them.\n",
    "- Ensure Compliance: Regulatory requirements often demand explainability.\n",
    "- Improve Models: Identifying weaknesses or biases in models.\n",
    "\n",
    "\n",
    "### 2.2 Introduction to SHAP Values\n",
    "Shapley values are a method from cooperative game theory used to explain the contribution of each feature to the prediction of a machine learning model, such as a neural network. They provide an interpretability technique by distributing the \"payout\" (the prediction) among the input features, attributing the importance of each feature to the prediction.\n",
    "\n",
    "For a given prediction, the Shapley value of a feature represents the average contribution of that feature to the prediction, considering all possible combinations of other features.\n",
    "\n",
    "1. **Marginal Contribution**:\n",
    "   The marginal contribution of a feature is the difference between the prediction with and without that feature.\n",
    "\n",
    "2. **Average over all subsets**:\n",
    "   The Shapley value is calculated by averaging the marginal contributions over all possible subsets of features.\n",
    "\n",
    "The formula for the Shapley value of feature $i$ is:\n",
    "\n",
    "$$\n",
    "\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N| - |S| - 1)!}{|N|!} \\cdot \\left( f(S \\cup \\{i\\}) - f(S) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the set of all features.\n",
    "- $S$ is a subset of features.\n",
    "- $f(S)$ is the model’s prediction when using only the features in subset $S$.\n",
    "\n",
    "\n",
    "The `shap` library is a popular tool for computing Shapley values for machine learning models, including neural networks.\n",
    "\n",
    "\n",
    "\n",
    "Why Use SHAP in RL?\n",
    "- Model-Agnostic: Applicable to any machine learning model, including neural networks.\n",
    "- Local Explanations: Provides explanations for individual predictions (actions).\n",
    "- Consistency: Ensures that features contributing more to the prediction have higher Shapley values.\n",
    "\n",
    "\n",
    "Properties of SHAP:\n",
    "1. Local Accuracy: The sum of Shapley values equals the difference between the model output and the expected output.\n",
    "2. Missingness: Features not present in the model have zero Shapley value.\n",
    "3. Consistency: If a model changes so that a feature contributes more to the prediction, the Shapley value of that feature should not decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade4682",
   "metadata": {
    "id": "21d49573"
   },
   "source": [
    "## 3. Calculating SHAP values <a name=\"calculating-shap-values\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3758646",
   "metadata": {
    "id": "d16712fc"
   },
   "source": [
    "We will work with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d5898",
   "metadata": {
    "id": "2c0c1262"
   },
   "source": [
    "- **Observations (`input_data`)**: These are the inputs to our actor neural network, representing the state of the environment.\n",
    "- **Trained Actor Model**: A neural network representing the decision making of one RL power plant that outputs actions based on the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77094d",
   "metadata": {
    "id": "2fb6fc14"
   },
   "source": [
    "Our goal is to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2fbc9",
   "metadata": {
    "id": "e0b69db6"
   },
   "source": [
    "1. Load the observations and the trained actor model.\n",
    "2. Use the model to predict actions.\n",
    "3. Apply SHAP to explain the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e0416",
   "metadata": {
    "id": "f870b3e8"
   },
   "source": [
    "### 3.1. Loading and Preparing Data <a name=\"loading-and-preparing-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b5965",
   "metadata": {
    "id": "aaa7c3d3"
   },
   "source": [
    "First, let's load the necessary libraries and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6f33b",
   "metadata": {
    "id": "b6ee4f28"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shap\n",
    "import torch as th\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f095c",
   "metadata": {
    "id": "ddfe95d9"
   },
   "source": [
    "We define a utility function to load observations and input data from a specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89d972",
   "metadata": {
    "id": "44862f06"
   },
   "outputs": [],
   "source": [
    "# @title Load observations function\n",
    "\n",
    "\n",
    "def load_observations(path, feature_names):\n",
    "    # Load observations\n",
    "    obs_path = f\"{path}/buffer_obs.json\"\n",
    "\n",
    "    with open(obs_path) as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # Convert the list of lists into a 2D numpy array\n",
    "    input_data = np.array(json_data)\n",
    "    input_data = np.squeeze(input_data)\n",
    "\n",
    "    # filter out arrays where all value are 0\n",
    "    input_data = input_data[~np.all(input_data == 0, axis=1)]\n",
    "\n",
    "    return pd.DataFrame(input_data, columns=feature_names), input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f6681",
   "metadata": {
    "id": "6c5f1986"
   },
   "source": [
    "**Load Observations and Input Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b084bb",
   "metadata": {
    "id": "36ae8f9e"
   },
   "source": [
    "Load the observations and input data using the utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d778759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to extra loggedobservation values\n",
    "path = input_dir + \"/learned_strategies/tutorial_08_zonal_case\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e142be2",
   "metadata": {
    "id": "d522969d"
   },
   "outputs": [],
   "source": [
    "# Define feature names (replace with actual feature names)\n",
    "# make columns names\n",
    "names_1 = [\"residual load forecast t+\" + str(x) for x in range(1, 13)]\n",
    "names_2 = [\"price forecast t+\" + str(x) for x in range(1, 13)]\n",
    "names_3 = [\"historical prices t+\" + str(x) for x in range(1, 13)]\n",
    "feature_names = (\n",
    "    names_1 + names_2 + names_3 + [\"total capacity t-1\"] + [\"marginal costs t-1\"]\n",
    ")\n",
    "\n",
    "df_obs, input_data = load_observations(path, feature_names)\n",
    "\n",
    "df_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add0715",
   "metadata": {
    "id": "5d8b9dcf"
   },
   "source": [
    "**Load the Trained Actor Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9e67d",
   "metadata": {
    "id": "b1b50488"
   },
   "source": [
    "We initialize and load the trained actor neural network. Therefore, we define the actor neural network class that will be used to predict actions based on observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca85e13",
   "metadata": {
    "id": "4da4de57"
   },
   "outputs": [],
   "source": [
    "from assume.reinforcement_learning.neural_network_architecture import MLPActor\n",
    "\n",
    "# Initialize the model\n",
    "obs_dim = len(feature_names)\n",
    "act_dim = 2  # Adjust if your model outputs a different number of actions\n",
    "model = MLPActor(obs_dim=obs_dim, act_dim=act_dim, float_type=th.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3b7e6",
   "metadata": {
    "id": "37adecfa"
   },
   "outputs": [],
   "source": [
    "# which actor is the RL actor\n",
    "ACTOR_NUM = len(powerplant_units)  # 20\n",
    "\n",
    "# Path to actor we want to analyse\n",
    "actor_path = os.path.join(\n",
    "    input_dir,\n",
    "    f\"learned_strategies/tutorial_08_zonal_case/avg_reward_eval_policies/actors/actor_Unit {ACTOR_NUM}.pt\",\n",
    ")\n",
    "\n",
    "# Load the trained model parameters\n",
    "model_state = th.load(actor_path, map_location=th.device(\"cpu\"))\n",
    "model.load_state_dict(model_state[\"actor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d43a1b",
   "metadata": {
    "id": "d4a63712"
   },
   "source": [
    "Get the actions base on observation tensor we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507d331",
   "metadata": {
    "id": "e6460cfb"
   },
   "outputs": [],
   "source": [
    "actions = []\n",
    "for obs in input_data:\n",
    "    obs_tensor = th.tensor(obs, dtype=th.float)\n",
    "    action = model(obs_tensor)\n",
    "    actions.append(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579665bc",
   "metadata": {
    "id": "ddd1ab1e"
   },
   "source": [
    "## 3.2. Creating a SHAP Explainer <a name=\"creating-a-shap-explainer\"></a>\n",
    "\n",
    "In the next step we create the Shap explainer. In this example we facilitat the Kernel Shap method. You can easily switch it out for Deep Shap. The SHAP Kernel Explainer is a model-agnostic method for computing SHAP values, which can be applied to any machine learning model, including black-box models like neural networks, decision trees, or ensemble models. It uses a simplified linear approximation based on the Kernel SHAP method to estimate the SHAP values, allowing you to interpret how each feature contributes to a particular model’s prediction. Basically the SHAP Kernel Explainer builds a weighted linear regression model around each prediction, using different combinations (coalitions) of input features to simulate their presence or absence. This results in SHAP values that represent the marginal contribution of each feature.\n",
    "\n",
    "As we fit a linear regression, we split the observatoin and action data into test and train data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0758eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ Title Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_data, actions, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to tensors\n",
    "y_train = th.stack(y_train)\n",
    "y_test = th.stack(y_test)\n",
    "\n",
    "X_train_tensor = th.tensor(X_train, dtype=th.float32)\n",
    "y_train_tensor = th.tensor(y_train, dtype=th.float32)\n",
    "X_test_tensor = th.tensor(X_test, dtype=th.float32)\n",
    "y_test_tensor = th.tensor(y_test, dtype=th.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaa45f",
   "metadata": {
    "id": "ae7b108b",
    "lines_to_next_cell": 2
   },
   "source": [
    "We define a prediction function compatible with SHAP and create a Kernel SHAP explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e12192",
   "metadata": {
    "id": "6d9be211"
   },
   "outputs": [],
   "source": [
    "# @ Title Define a prediction function for generating actions for SHAP Explainer\n",
    "def model_predict(X):\n",
    "    X_tensor = th.tensor(X, dtype=th.float32)\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        return model(X_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a32f41",
   "metadata": {
    "id": "84bb96cf"
   },
   "outputs": [],
   "source": [
    "# Create the SHAP Kernel Explainer\n",
    "explainer = shap.KernelExplainer(model_predict, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279910b",
   "metadata": {
    "id": "2a7929e4"
   },
   "outputs": [],
   "source": [
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ca286",
   "metadata": {
    "id": "c1f3d550"
   },
   "source": [
    "## 4. Visualizing SHAP Values <a name=\"visualizing-shap-values\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cecbfe",
   "metadata": {
    "id": "3a0f0cbe"
   },
   "source": [
    "We generate summary plots to visualize feature importance for each output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3d533",
   "metadata": {
    "id": "2e318a5b"
   },
   "outputs": [],
   "source": [
    "# Summary plot for the first output dimension\n",
    "shap.summary_plot(shap_values[:, :, 0], X_test, feature_names=feature_names, show=False)\n",
    "plt.title(\"Summary Plot for Output Dimension 0, p_inflex\")\n",
    "plt.show()\n",
    "\n",
    "# Summary plot for the second output dimension\n",
    "shap.summary_plot(shap_values[:, :, 1], X_test, feature_names=feature_names, show=False)\n",
    "plt.title(\"Summary Plot for Output Dimension 1, p_flex\")\n",
    "plt.show()\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values[:, :, 0],\n",
    "    X_test,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Summary Bar Plot for Output Dimension 0\",\n",
    ")\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values[:, :, 1],\n",
    "    X_test,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Summary Bar Plot for Output Dimension 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7ebfb",
   "metadata": {
    "id": "9a888f8b"
   },
   "source": [
    "The SHAP summary plots show the impact of each feature on the model's predictions for each output dimension (action). Features with larger absolute SHAP values have a more significant influence on the decision-making process of the RL agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555afa84",
   "metadata": {
    "id": "c6c4ce8c"
   },
   "source": [
    "- **Positive SHAP Value**: Indicates that the feature contributes positively to the predicted action value.\n",
    "- **Negative SHAP Value**: Indicates that the feature contributes negatively to the predicted action value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc307ed",
   "metadata": {
    "id": "86545200"
   },
   "source": [
    "By analyzing these plots, we can identify which features are most influential and understand how changes in feature values affect the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b57e2",
   "metadata": {
    "id": "06f3977c"
   },
   "source": [
    "## 5. Conclusion <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79634a",
   "metadata": {
    "id": "dadd0a0c"
   },
   "source": [
    "In this tutorial, we've demonstrated how to apply SHAP to a reinforcement learning agent to explain its decision-making process. By interpreting the SHAP values, we gain valuable insights into which features influence the agent's actions, enhancing transparency and trust in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0035820",
   "metadata": {
    "id": "37633c16"
   },
   "source": [
    "Explainability is crucial, especially when deploying RL agents in real-world applications where understanding the rationale behind decisions is essential for safety, fairness, and compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c10373",
   "metadata": {
    "id": "8735d66f"
   },
   "source": [
    "## 6. Additional Resources <a name=\"additional-resources\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80754bea",
   "metadata": {
    "id": "d6b0332f"
   },
   "source": [
    "- **SHAP Documentation**: [https://shap.readthedocs.io/en/latest/](https://shap.readthedocs.io/en/latest/)\n",
    "- **PyTorch Documentation**: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
    "- **Reinforcement Learning Introduction**: [Richard S. Sutton and Andrew G. Barto, \"Reinforcement Learning: An Introduction\"](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "- **Interpretable Machine Learning Book**: [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b873097",
   "metadata": {
    "id": "a8cdea5f"
   },
   "source": [
    "**Feel free to experiment with the code and explore different explainability techniques. Happy learning!**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "vscode,outputId,colab,cellView,id,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
