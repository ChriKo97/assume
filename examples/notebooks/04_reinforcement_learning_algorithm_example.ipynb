{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JeBorbE6FYr"
   },
   "source": [
    "# 4. RL Algorithm tutorial\n",
    "\n",
    "This tutorial will introduce users into the MATD3 implementation in ASSUME and hence how we use reinforcement leanring (RL). The main objective of this tutorial is to ensure participants grasp the steps required to equip ASSUME with a RL alogorithm. It ,therefore, start one level deeper, than the RL_application example and the knowledge from this tutorial is not required, if the already perconfigured algorithm in Assume should be used. The algorithm explained here is usable as a plug and play solution in the framework. The following coding snippets will highlight the key in the algorithm class and will explain the interactions with the learning role and other classes along the way. \n",
    "\n",
    "The outline of this tutorial is as follows. We will start with an introduction to the changed simualtion flow when we use reinforcement learning (1. From one simulation year to learning episodes). If you need a refresher on RL in general, please visit our readthedocs (https://assume.readthedocs.io/en/latest/). Afterwards, we dive into the tasks and reason behind a learning role (2. What role has a learning role) and then dive into the characteristics of the algorithm (3. The MATD3).\n",
    "\n",
    "**Please Note:** The tutorial does not cover coding tasks. It simply provides an overview and explanation of the implementation of reinforcement learning and the flow for those who would like to modify the underlying learning algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Assume\n",
    "\n",
    "Frist we need to install Assume in this Colab. Here we just install the ASSUME core package via pip. In general the instructions for an installation can be found here: https://assume.readthedocs.io/en/latest/installation.html. All the required steps are executed here and since we are working in colab the generation of a venv is not necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0DaRwFA7VgW",
    "outputId": "5655adad-5b7a-4fe3-9067-6b502a06136b",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: assume-framework in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: argcomplete>=3.1.4 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (3.4.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.6 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (1.6.0)\n",
      "Requirement already satisfied: mango-agents-assume>=1.1.1-8 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (1.1.4.post2)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (4.66.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (2.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.9 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (2.0.30)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (2.2.2)\n",
      "Requirement already satisfied: psycopg2-binary>=2.9.5 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (2.9.9)\n",
      "Requirement already satisfied: pyyaml>=6.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (6.0.1)\n",
      "Requirement already satisfied: pyyaml-include>=1.3.1 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from assume-framework) (1.4.1)\n",
      "Requirement already satisfied: paho-mqtt>=1.5.1 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from mango-agents-assume>=1.1.1-8->assume-framework) (2.1.0)\n",
      "Requirement already satisfied: dill>=0.3.6 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from mango-agents-assume>=1.1.1-8->assume-framework) (0.3.8)\n",
      "Requirement already satisfied: msgspec>=0.14.2 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from mango-agents-assume>=1.1.1-8->assume-framework) (0.18.6)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from mango-agents-assume>=1.1.1-8->assume-framework) (5.27.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from pandas>=2.0.0->assume-framework) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from pandas>=2.0.0->assume-framework) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from python-dateutil>=2.8.2->assume-framework) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from sqlalchemy>=2.0.9->assume-framework) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from sqlalchemy>=2.0.9->assume-framework) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tg3533\\appdata\\local\\miniconda3\\envs\\assume-framework\\lib\\site-packages (from tqdm>=4.64.1->assume-framework) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install assume-framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIw_QIE3pY34"
   },
   "source": [
    "And easy like this we have ASSUME installed. Now we can let it run. Please note though that we cannot use the functionalities tied to docker and, hence, cannot access the predefined dashboards in colab. For this please install docker and ASSUME on your personal machine.\n",
    "\n",
    "Further we would like to access the predefined scenarios in ASSUME which are stored on the git repository. Hence, we clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5hB0uDisSsg",
    "outputId": "1241881f-e090-4f26-9b02-560adfcb3a3e",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'assume'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/assume-framework/assume.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg7DyNjLuvSb"
   },
   "source": [
    "**Let the magic happen.** Now you can run your first ever simulation in ASSUME. The following code navigates to the respective assume folder and starts the simulation example example_01b using the local database here in colab.\n",
    "\n",
    "When running locally, you can also just run `assume -s example_01b -db \"sqlite:///./examples/local_db/assume_db_example_01b.db\"` in a shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eVM60Qx8SC0",
    "outputId": "20434515-6e65-4d34-d44d-8c4529a46ece",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.world:connected to db\n",
      "INFO:assume.scenario.loader_csv:Starting Scenario example_01b/ from examples/inputs\n",
      "INFO:assume.scenario.loader_csv:storage_units not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:industrial_dsm_units not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:forecasts_df not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:Downsampling demand_df successful.\n",
      "INFO:assume.scenario.loader_csv:cross_border_flows not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:Downsampling availability_df successful.\n",
      "INFO:assume.scenario.loader_csv:electricity_prices not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:price_forecasts not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:temperature not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2678400 [00:00<?, ?it/s]\n",
      "example_01b_base 2019-01-01 00:00:00:   0%|          | 3601/2678400 [00:00<01:27, 30423.20it/s]\n",
      "example_01b_base 2019-01-01 02:00:00:   0%|          | 10801/2678400 [00:00<01:14, 35684.71it/s]\n",
      "example_01b_base 2019-01-01 04:00:00:   1%|          | 18001/2678400 [00:00<01:00, 43879.71it/s]\n",
      "example_01b_base 2019-01-01 06:00:00:   1%|          | 25201/2678400 [00:00<00:54, 48916.83it/s]\n",
      "example_01b_base 2019-01-01 08:00:00:   1%|          | 32401/2678400 [00:00<00:50, 52321.35it/s]\n",
      "example_01b_base 2019-01-01 10:00:00:   1%|▏         | 39601/2678400 [00:00<00:48, 54677.16it/s]\n",
      "example_01b_base 2019-01-01 12:00:00:   2%|▏         | 46801/2678400 [00:00<00:46, 56015.40it/s]\n",
      "example_01b_base 2019-01-01 14:00:00:   2%|▏         | 54001/2678400 [00:01<00:46, 56438.41it/s]\n",
      "example_01b_base 2019-01-01 16:00:00:   2%|▏         | 61201/2678400 [00:01<00:45, 57369.36it/s]\n",
      "example_01b_base 2019-01-01 18:00:00:   3%|▎         | 68401/2678400 [00:01<00:45, 57918.05it/s]\n",
      "example_01b_base 2019-01-01 20:00:00:   3%|▎         | 75601/2678400 [00:01<00:44, 58782.83it/s]\n",
      "example_01b_base 2019-01-01 22:00:00:   3%|▎         | 82801.0/2678400 [00:01<00:55, 46651.59it/s]\n",
      "example_01b_base 2019-01-02 00:00:00:   3%|▎         | 90001.0/2678400 [00:01<00:53, 48532.18it/s]\n",
      "example_01b_base 2019-01-02 02:00:00:   4%|▎         | 97201.0/2678400 [00:01<00:51, 49946.25it/s]\n",
      "example_01b_base 2019-01-02 04:00:00:   4%|▍         | 104401.0/2678400 [00:02<00:51, 49760.00it/s]\n",
      "example_01b_base 2019-01-02 06:00:00:   4%|▍         | 111601.0/2678400 [00:02<00:51, 50168.93it/s]\n",
      "example_01b_base 2019-01-02 08:00:00:   4%|▍         | 118801.0/2678400 [00:02<00:51, 49930.61it/s]\n",
      "example_01b_base 2019-01-02 10:00:00:   5%|▍         | 126001.0/2678400 [00:02<00:49, 51261.82it/s]\n",
      "example_01b_base 2019-01-02 12:00:00:   5%|▍         | 133201.0/2678400 [00:02<00:47, 53167.22it/s]\n",
      "example_01b_base 2019-01-02 14:00:00:   5%|▌         | 140401.0/2678400 [00:02<00:47, 53969.30it/s]\n",
      "example_01b_base 2019-01-02 16:00:00:   6%|▌         | 147601.0/2678400 [00:02<00:45, 55224.72it/s]\n",
      "example_01b_base 2019-01-02 18:00:00:   6%|▌         | 154801.0/2678400 [00:02<00:44, 56201.60it/s]\n",
      "example_01b_base 2019-01-02 20:00:00:   6%|▌         | 162001.0/2678400 [00:03<00:44, 56782.43it/s]\n",
      "example_01b_base 2019-01-02 22:00:00:   6%|▋         | 169201.0/2678400 [00:03<00:52, 47651.02it/s]\n",
      "example_01b_base 2019-01-03 00:00:00:   7%|▋         | 176401.0/2678400 [00:03<00:49, 50330.03it/s]\n",
      "example_01b_base 2019-01-03 02:00:00:   7%|▋         | 183601.0/2678400 [00:03<00:47, 52560.30it/s]\n",
      "example_01b_base 2019-01-03 04:00:00:   7%|▋         | 190801.0/2678400 [00:03<00:45, 54478.59it/s]\n",
      "example_01b_base 2019-01-03 06:00:00:   7%|▋         | 198001.0/2678400 [00:03<00:44, 55873.16it/s]\n",
      "example_01b_base 2019-01-03 08:00:00:   8%|▊         | 205201.0/2678400 [00:03<00:43, 56714.95it/s]\n",
      "example_01b_base 2019-01-03 10:00:00:   8%|▊         | 212401.0/2678400 [00:04<00:51, 47916.31it/s]\n",
      "example_01b_base 2019-01-03 12:00:00:   8%|▊         | 219601.0/2678400 [00:04<00:48, 50684.66it/s]\n",
      "example_01b_base 2019-01-03 14:00:00:   8%|▊         | 226801.0/2678400 [00:04<00:46, 52841.65it/s]\n",
      "example_01b_base 2019-01-03 16:00:00:   9%|▊         | 234001.0/2678400 [00:04<00:45, 54276.38it/s]\n",
      "example_01b_base 2019-01-03 18:00:00:   9%|▉         | 241201.0/2678400 [00:04<00:43, 55719.08it/s]\n",
      "example_01b_base 2019-01-03 20:00:00:   9%|▉         | 248401.0/2678400 [00:04<00:42, 56707.21it/s]\n",
      "example_01b_base 2019-01-03 22:00:00:  10%|▉         | 255601.0/2678400 [00:04<00:49, 48519.06it/s]\n",
      "example_01b_base 2019-01-04 00:00:00:  10%|▉         | 262801.0/2678400 [00:05<00:47, 51120.55it/s]\n",
      "example_01b_base 2019-01-04 02:00:00:  10%|█         | 270001.0/2678400 [00:05<00:45, 53096.97it/s]\n",
      "example_01b_base 2019-01-04 04:00:00:  10%|█         | 277201.0/2678400 [00:05<00:44, 54321.62it/s]\n",
      "example_01b_base 2019-01-04 06:00:00:  11%|█         | 284401.0/2678400 [00:05<00:43, 55360.47it/s]\n",
      "example_01b_base 2019-01-04 08:00:00:  11%|█         | 291601.0/2678400 [00:05<00:42, 55920.13it/s]\n",
      "example_01b_base 2019-01-04 10:00:00:  11%|█         | 298801.0/2678400 [00:05<00:42, 56113.39it/s]\n",
      "example_01b_base 2019-01-04 12:00:00:  11%|█▏        | 306001.0/2678400 [00:05<00:41, 56590.39it/s]\n",
      "example_01b_base 2019-01-04 14:00:00:  12%|█▏        | 313201.0/2678400 [00:05<00:41, 56928.53it/s]\n",
      "example_01b_base 2019-01-04 16:00:00:  12%|█▏        | 320401.0/2678400 [00:06<00:41, 57118.79it/s]\n",
      "example_01b_base 2019-01-04 18:00:00:  12%|█▏        | 327601.0/2678400 [00:06<00:40, 57575.25it/s]\n",
      "example_01b_base 2019-01-04 20:00:00:  13%|█▎        | 334801.0/2678400 [00:06<00:40, 58112.25it/s]\n",
      "example_01b_base 2019-01-04 22:00:00:  13%|█▎        | 342001.0/2678400 [00:06<00:48, 48367.70it/s]\n",
      "example_01b_base 2019-01-05 00:00:00:  13%|█▎        | 349201.0/2678400 [00:06<00:45, 51168.60it/s]\n",
      "example_01b_base 2019-01-05 02:00:00:  13%|█▎        | 356401.0/2678400 [00:06<00:43, 53641.90it/s]\n",
      "example_01b_base 2019-01-05 04:00:00:  14%|█▎        | 363601.0/2678400 [00:06<00:41, 55441.05it/s]\n",
      "example_01b_base 2019-01-05 06:00:00:  14%|█▍        | 370801.0/2678400 [00:06<00:41, 55458.69it/s]\n",
      "example_01b_base 2019-01-05 08:00:00:  14%|█▍        | 378001.0/2678400 [00:07<00:43, 53162.53it/s]\n",
      "example_01b_base 2019-01-05 10:00:00:  14%|█▍        | 385201.0/2678400 [00:07<00:44, 51707.84it/s]\n",
      "example_01b_base 2019-01-05 12:00:00:  15%|█▍        | 392401.0/2678400 [00:07<00:45, 50168.86it/s]\n",
      "example_01b_base 2019-01-05 14:00:00:  15%|█▍        | 399601.0/2678400 [00:07<00:46, 49157.72it/s]\n",
      "example_01b_base 2019-01-05 16:00:00:  15%|█▌        | 406801.0/2678400 [00:07<00:44, 51156.05it/s]\n",
      "example_01b_base 2019-01-05 18:00:00:  15%|█▌        | 414001.0/2678400 [00:07<00:42, 53232.21it/s]\n",
      "example_01b_base 2019-01-05 20:00:00:  16%|█▌        | 421201.0/2678400 [00:08<00:49, 45728.43it/s]\n",
      "example_01b_base 2019-01-05 22:00:00:  16%|█▌        | 428401.0/2678400 [00:08<00:53, 42063.88it/s]\n",
      "example_01b_base 2019-01-06 00:00:00:  16%|█▋        | 435601.0/2678400 [00:08<00:48, 45834.91it/s]\n",
      "example_01b_base 2019-01-06 02:00:00:  17%|█▋        | 442801.0/2678400 [00:08<00:45, 48804.88it/s]\n",
      "example_01b_base 2019-01-06 04:00:00:  17%|█▋        | 450001.0/2678400 [00:08<00:43, 51342.78it/s]\n",
      "example_01b_base 2019-01-06 06:00:00:  17%|█▋        | 457201.0/2678400 [00:08<00:41, 52989.00it/s]\n",
      "example_01b_base 2019-01-06 08:00:00:  17%|█▋        | 464401.0/2678400 [00:08<00:40, 54441.28it/s]\n",
      "example_01b_base 2019-01-06 10:00:00:  18%|█▊        | 471601.0/2678400 [00:09<00:39, 55671.84it/s]\n",
      "example_01b_base 2019-01-06 12:00:00:  18%|█▊        | 478801.0/2678400 [00:09<00:39, 56140.84it/s]\n",
      "example_01b_base 2019-01-06 14:00:00:  18%|█▊        | 486001.0/2678400 [00:09<00:39, 56187.50it/s]\n",
      "example_01b_base 2019-01-06 16:00:00:  18%|█▊        | 493201.0/2678400 [00:09<00:38, 56440.00it/s]\n",
      "example_01b_base 2019-01-06 18:00:00:  19%|█▊        | 500401.0/2678400 [00:09<00:38, 56687.77it/s]\n",
      "example_01b_base 2019-01-06 20:00:00:  19%|█▉        | 507601.0/2678400 [00:09<00:38, 56995.87it/s]\n",
      "example_01b_base 2019-01-06 22:00:00:  19%|█▉        | 514801.0/2678400 [00:09<00:45, 47951.17it/s]\n",
      "example_01b_base 2019-01-07 00:00:00:  19%|█▉        | 522001.0/2678400 [00:09<00:42, 50964.31it/s]\n",
      "example_01b_base 2019-01-07 02:00:00:  20%|█▉        | 529201.0/2678400 [00:10<00:40, 53280.58it/s]\n",
      "example_01b_base 2019-01-07 04:00:00:  20%|██        | 536401.0/2678400 [00:10<00:39, 54604.39it/s]\n",
      "example_01b_base 2019-01-07 06:00:00:  20%|██        | 543601.0/2678400 [00:10<00:38, 55587.37it/s]\n",
      "example_01b_base 2019-01-07 08:00:00:  21%|██        | 550801.0/2678400 [00:10<00:37, 56928.63it/s]\n",
      "example_01b_base 2019-01-07 10:00:00:  21%|██        | 558001.0/2678400 [00:10<00:37, 55904.47it/s]\n",
      "example_01b_base 2019-01-07 12:00:00:  21%|██        | 565201.0/2678400 [00:10<00:37, 56934.76it/s]\n",
      "example_01b_base 2019-01-07 14:00:00:  21%|██▏       | 572401.0/2678400 [00:10<00:36, 57832.91it/s]\n",
      "example_01b_base 2019-01-07 16:00:00:  22%|██▏       | 579601.0/2678400 [00:10<00:36, 56962.96it/s]\n",
      "example_01b_base 2019-01-07 18:00:00:  22%|██▏       | 586801.0/2678400 [00:11<00:36, 57476.44it/s]\n",
      "example_01b_base 2019-01-07 20:00:00:  22%|██▏       | 594001.0/2678400 [00:11<00:35, 57953.54it/s]\n",
      "example_01b_base 2019-01-07 22:00:00:  22%|██▏       | 601201.0/2678400 [00:11<00:42, 48611.34it/s]\n",
      "example_01b_base 2019-01-08 00:00:00:  23%|██▎       | 608401.0/2678400 [00:11<00:46, 44520.51it/s]\n",
      "example_01b_base 2019-01-08 02:00:00:  23%|██▎       | 615601.0/2678400 [00:11<00:43, 47465.15it/s]\n",
      "example_01b_base 2019-01-08 04:00:00:  23%|██▎       | 622801.0/2678400 [00:11<00:40, 50625.93it/s]\n",
      "example_01b_base 2019-01-08 06:00:00:  24%|██▎       | 630001.0/2678400 [00:11<00:38, 52659.35it/s]\n",
      "example_01b_base 2019-01-08 08:00:00:  24%|██▍       | 637201.0/2678400 [00:12<00:37, 54379.30it/s]\n",
      "example_01b_base 2019-01-08 10:00:00:  24%|██▍       | 644401.0/2678400 [00:12<00:36, 55208.37it/s]\n",
      "example_01b_base 2019-01-08 12:00:00:  24%|██▍       | 651601.0/2678400 [00:12<00:36, 55401.90it/s]\n",
      "example_01b_base 2019-01-08 14:00:00:  25%|██▍       | 658801.0/2678400 [00:12<00:39, 51483.72it/s]\n",
      "example_01b_base 2019-01-08 16:00:00:  25%|██▍       | 666001.0/2678400 [00:12<00:40, 50061.37it/s]\n",
      "example_01b_base 2019-01-08 18:00:00:  25%|██▌       | 673201.0/2678400 [00:12<00:43, 46256.51it/s]\n",
      "example_01b_base 2019-01-08 20:00:00:  25%|██▌       | 680401.0/2678400 [00:12<00:40, 49264.48it/s]\n",
      "example_01b_base 2019-01-08 22:00:00:  26%|██▌       | 687601.0/2678400 [00:13<00:44, 44394.86it/s]\n",
      "example_01b_base 2019-01-09 00:00:00:  26%|██▌       | 694801.0/2678400 [00:13<00:41, 47699.61it/s]\n",
      "example_01b_base 2019-01-09 02:00:00:  26%|██▌       | 702001.0/2678400 [00:13<00:41, 47194.71it/s]\n",
      "example_01b_base 2019-01-09 04:00:00:  26%|██▋       | 709201.0/2678400 [00:13<00:39, 49717.87it/s]\n",
      "example_01b_base 2019-01-09 06:00:00:  27%|██▋       | 716401.0/2678400 [00:13<00:37, 51893.99it/s]\n",
      "example_01b_base 2019-01-09 08:00:00:  27%|██▋       | 723601.0/2678400 [00:13<00:36, 53769.22it/s]\n",
      "example_01b_base 2019-01-09 10:00:00:  27%|██▋       | 730801.0/2678400 [00:13<00:35, 54930.12it/s]\n",
      "example_01b_base 2019-01-09 12:00:00:  28%|██▊       | 738001.0/2678400 [00:14<00:34, 55946.74it/s]\n",
      "example_01b_base 2019-01-09 14:00:00:  28%|██▊       | 745201.0/2678400 [00:14<00:34, 56055.56it/s]\n",
      "example_01b_base 2019-01-09 16:00:00:  28%|██▊       | 752401.0/2678400 [00:14<00:34, 56466.57it/s]\n",
      "example_01b_base 2019-01-09 18:00:00:  28%|██▊       | 759601.0/2678400 [00:14<00:33, 57112.01it/s]\n",
      "example_01b_base 2019-01-09 20:00:00:  29%|██▊       | 766801.0/2678400 [00:14<00:33, 57367.57it/s]\n",
      "example_01b_base 2019-01-09 22:00:00:  29%|██▉       | 774001.0/2678400 [00:14<00:39, 48196.01it/s]\n",
      "example_01b_base 2019-01-10 00:00:00:  29%|██▉       | 781201.0/2678400 [00:14<00:37, 50952.27it/s]\n",
      "example_01b_base 2019-01-10 02:00:00:  29%|██▉       | 788401.0/2678400 [00:15<00:35, 53072.46it/s]\n",
      "example_01b_base 2019-01-10 04:00:00:  30%|██▉       | 795601.0/2678400 [00:15<00:34, 54740.47it/s]\n",
      "example_01b_base 2019-01-10 06:00:00:  30%|██▉       | 802801.0/2678400 [00:15<00:33, 55748.34it/s]\n",
      "example_01b_base 2019-01-10 08:00:00:  30%|███       | 810001.0/2678400 [00:15<00:33, 56452.47it/s]\n",
      "example_01b_base 2019-01-10 10:00:00:  31%|███       | 817201.0/2678400 [00:15<00:39, 46885.25it/s]\n",
      "example_01b_base 2019-01-10 12:00:00:  31%|███       | 824401.0/2678400 [00:15<00:37, 49855.61it/s]\n",
      "example_01b_base 2019-01-10 14:00:00:  31%|███       | 831601.0/2678400 [00:15<00:35, 52171.75it/s]\n",
      "example_01b_base 2019-01-10 16:00:00:  31%|███▏      | 838801.0/2678400 [00:15<00:34, 53783.65it/s]\n",
      "example_01b_base 2019-01-10 18:00:00:  32%|███▏      | 846001.0/2678400 [00:16<00:33, 55235.28it/s]\n",
      "example_01b_base 2019-01-10 20:00:00:  32%|███▏      | 853201.0/2678400 [00:16<00:32, 56718.68it/s]\n",
      "example_01b_base 2019-01-10 22:00:00:  32%|███▏      | 860401.0/2678400 [00:16<00:37, 48428.40it/s]\n",
      "example_01b_base 2019-01-11 00:00:00:  32%|███▏      | 867601.0/2678400 [00:16<00:35, 50738.92it/s]\n",
      "example_01b_base 2019-01-11 02:00:00:  33%|███▎      | 874801.0/2678400 [00:16<00:33, 53173.89it/s]\n",
      "example_01b_base 2019-01-11 04:00:00:  33%|███▎      | 882001.0/2678400 [00:16<00:32, 55119.72it/s]\n",
      "example_01b_base 2019-01-11 06:00:00:  33%|███▎      | 889201.0/2678400 [00:16<00:33, 53537.09it/s]\n",
      "example_01b_base 2019-01-11 08:00:00:  33%|███▎      | 896401.0/2678400 [00:17<00:33, 52684.28it/s]\n",
      "example_01b_base 2019-01-11 10:00:00:  34%|███▎      | 903601.0/2678400 [00:17<00:33, 53023.61it/s]\n",
      "example_01b_base 2019-01-11 12:00:00:  34%|███▍      | 910801.0/2678400 [00:17<00:33, 53014.08it/s]\n",
      "example_01b_base 2019-01-11 14:00:00:  34%|███▍      | 918001.0/2678400 [00:17<00:36, 48425.12it/s]\n",
      "example_01b_base 2019-01-11 16:00:00:  35%|███▍      | 925201.0/2678400 [00:17<00:41, 42470.72it/s]\n",
      "example_01b_base 2019-01-11 18:00:00:  35%|███▍      | 932401.0/2678400 [00:17<00:41, 42097.49it/s]\n",
      "example_01b_base 2019-01-11 20:00:00:  35%|███▌      | 939601.0/2678400 [00:18<00:40, 43376.09it/s]\n",
      "example_01b_base 2019-01-11 22:00:00:  35%|███▌      | 946801.0/2678400 [00:18<00:43, 39920.13it/s]\n",
      "example_01b_base 2019-01-12 00:00:00:  36%|███▌      | 954001.0/2678400 [00:18<00:40, 42524.99it/s]\n",
      "example_01b_base 2019-01-12 02:00:00:  36%|███▌      | 961201.0/2678400 [00:18<00:38, 44660.42it/s]\n",
      "example_01b_base 2019-01-12 04:00:00:  36%|███▌      | 968401.0/2678400 [00:18<00:36, 47160.31it/s]\n",
      "example_01b_base 2019-01-12 06:00:00:  36%|███▋      | 975601.0/2678400 [00:18<00:33, 50092.75it/s]\n",
      "example_01b_base 2019-01-12 08:00:00:  37%|███▋      | 982801.0/2678400 [00:18<00:32, 52603.16it/s]\n",
      "example_01b_base 2019-01-12 10:00:00:  37%|███▋      | 990001.0/2678400 [00:19<00:31, 53164.04it/s]\n",
      "example_01b_base 2019-01-12 12:00:00:  37%|███▋      | 997201.0/2678400 [00:19<00:30, 54344.62it/s]\n",
      "example_01b_base 2019-01-12 14:00:00:  38%|███▊      | 1004401.0/2678400 [00:19<00:30, 55660.78it/s]\n",
      "example_01b_base 2019-01-12 16:00:00:  38%|███▊      | 1011601.0/2678400 [00:19<00:29, 56636.65it/s]\n",
      "example_01b_base 2019-01-12 18:00:00:  38%|███▊      | 1018801.0/2678400 [00:19<00:29, 56636.45it/s]\n",
      "example_01b_base 2019-01-12 20:00:00:  38%|███▊      | 1026001.0/2678400 [00:19<00:28, 57714.48it/s]\n",
      "example_01b_base 2019-01-12 22:00:00:  39%|███▊      | 1033201.0/2678400 [00:19<00:39, 41666.08it/s]\n",
      "example_01b_base 2019-01-13 00:00:00:  39%|███▉      | 1040401.0/2678400 [00:20<00:36, 45168.24it/s]\n",
      "example_01b_base 2019-01-13 02:00:00:  39%|███▉      | 1047601.0/2678400 [00:20<00:33, 48069.22it/s]\n",
      "example_01b_base 2019-01-13 04:00:00:  39%|███▉      | 1054801.0/2678400 [00:20<00:31, 50878.73it/s]\n",
      "example_01b_base 2019-01-13 06:00:00:  40%|███▉      | 1062001.0/2678400 [00:20<00:30, 52964.73it/s]\n",
      "example_01b_base 2019-01-13 08:00:00:  40%|███▉      | 1069201.0/2678400 [00:20<00:32, 50077.34it/s]\n",
      "example_01b_base 2019-01-13 10:00:00:  40%|████      | 1076401.0/2678400 [00:20<00:32, 49534.87it/s]\n",
      "example_01b_base 2019-01-13 12:00:00:  40%|████      | 1083601.0/2678400 [00:20<00:31, 50580.14it/s]\n",
      "example_01b_base 2019-01-13 14:00:00:  41%|████      | 1090801.0/2678400 [00:21<00:30, 51331.43it/s]\n",
      "example_01b_base 2019-01-13 16:00:00:  41%|████      | 1098001.0/2678400 [00:21<00:30, 51689.05it/s]\n",
      "example_01b_base 2019-01-13 18:00:00:  41%|████▏     | 1105201.0/2678400 [00:21<00:30, 51371.33it/s]\n",
      "example_01b_base 2019-01-13 20:00:00:  42%|████▏     | 1112401.0/2678400 [00:21<00:32, 47756.43it/s]\n",
      "example_01b_base 2019-01-13 22:00:00:  42%|████▏     | 1119601.0/2678400 [00:21<00:36, 42458.69it/s]\n",
      "example_01b_base 2019-01-14 00:00:00:  42%|████▏     | 1126801.0/2678400 [00:21<00:33, 46246.11it/s]\n",
      "example_01b_base 2019-01-14 02:00:00:  42%|████▏     | 1134001.0/2678400 [00:21<00:31, 49474.99it/s]\n",
      "example_01b_base 2019-01-14 04:00:00:  43%|████▎     | 1141201.0/2678400 [00:22<00:29, 52176.46it/s]\n",
      "example_01b_base 2019-01-14 06:00:00:  43%|████▎     | 1148401.0/2678400 [00:22<00:28, 53639.43it/s]\n",
      "example_01b_base 2019-01-14 08:00:00:  43%|████▎     | 1155601.0/2678400 [00:22<00:27, 54775.44it/s]\n",
      "example_01b_base 2019-01-14 10:00:00:  43%|████▎     | 1162801.0/2678400 [00:22<00:27, 56038.72it/s]\n",
      "example_01b_base 2019-01-14 12:00:00:  44%|████▎     | 1170001.0/2678400 [00:22<00:27, 55525.94it/s]\n",
      "example_01b_base 2019-01-14 14:00:00:  44%|████▍     | 1177201.0/2678400 [00:22<00:27, 55088.51it/s]\n",
      "example_01b_base 2019-01-14 16:00:00:  44%|████▍     | 1184401.0/2678400 [00:22<00:27, 54337.91it/s]\n",
      "example_01b_base 2019-01-14 18:00:00:  44%|████▍     | 1191601.0/2678400 [00:23<00:28, 52248.87it/s]\n",
      "example_01b_base 2019-01-14 20:00:00:  45%|████▍     | 1198801.0/2678400 [00:23<00:28, 51591.58it/s]\n",
      "example_01b_base 2019-01-14 22:00:00:  45%|████▌     | 1206001.0/2678400 [00:23<00:33, 43631.98it/s]\n",
      "example_01b_base 2019-01-15 00:00:00:  45%|████▌     | 1213201.0/2678400 [00:23<00:31, 47216.11it/s]\n",
      "example_01b_base 2019-01-15 02:00:00:  46%|████▌     | 1220401.0/2678400 [00:23<00:34, 42328.00it/s]\n",
      "example_01b_base 2019-01-15 04:00:00:  46%|████▌     | 1227601.0/2678400 [00:23<00:31, 46346.74it/s]\n",
      "example_01b_base 2019-01-15 06:00:00:  46%|████▌     | 1234801.0/2678400 [00:23<00:29, 49383.81it/s]\n",
      "example_01b_base 2019-01-15 08:00:00:  46%|████▋     | 1242001.0/2678400 [00:24<00:27, 51554.85it/s]\n",
      "example_01b_base 2019-01-15 10:00:00:  47%|████▋     | 1249201.0/2678400 [00:24<00:26, 53330.32it/s]\n",
      "example_01b_base 2019-01-15 12:00:00:  47%|████▋     | 1256401.0/2678400 [00:24<00:25, 54848.79it/s]\n",
      "example_01b_base 2019-01-15 14:00:00:  47%|████▋     | 1263601.0/2678400 [00:24<00:25, 55445.65it/s]\n",
      "example_01b_base 2019-01-15 16:00:00:  47%|████▋     | 1270801.0/2678400 [00:24<00:24, 56450.87it/s]\n",
      "example_01b_base 2019-01-15 18:00:00:  48%|████▊     | 1278001.0/2678400 [00:24<00:24, 56981.82it/s]\n",
      "example_01b_base 2019-01-15 20:00:00:  48%|████▊     | 1285201.0/2678400 [00:24<00:24, 56716.50it/s]\n",
      "example_01b_base 2019-01-15 22:00:00:  48%|████▊     | 1292401.0/2678400 [00:25<00:28, 48578.09it/s]\n",
      "example_01b_base 2019-01-16 00:00:00:  49%|████▊     | 1299601.0/2678400 [00:25<00:27, 50879.06it/s]\n",
      "example_01b_base 2019-01-16 02:00:00:  49%|████▉     | 1306801.0/2678400 [00:25<00:25, 53058.53it/s]\n",
      "example_01b_base 2019-01-16 04:00:00:  49%|████▉     | 1314001.0/2678400 [00:25<00:25, 54402.64it/s]\n",
      "example_01b_base 2019-01-16 06:00:00:  49%|████▉     | 1321201.0/2678400 [00:25<00:24, 55269.63it/s]\n",
      "example_01b_base 2019-01-16 08:00:00:  50%|████▉     | 1328401.0/2678400 [00:25<00:24, 55756.53it/s]\n",
      "example_01b_base 2019-01-16 10:00:00:  50%|████▉     | 1335601.0/2678400 [00:25<00:23, 56358.90it/s]\n",
      "example_01b_base 2019-01-16 12:00:00:  50%|█████     | 1342801.0/2678400 [00:25<00:23, 57446.34it/s]\n",
      "example_01b_base 2019-01-16 14:00:00:  50%|█████     | 1350001.0/2678400 [00:26<00:23, 57650.40it/s]\n",
      "example_01b_base 2019-01-16 16:00:00:  51%|█████     | 1357201.0/2678400 [00:26<00:23, 56951.13it/s]\n",
      "example_01b_base 2019-01-16 18:00:00:  51%|█████     | 1364401.0/2678400 [00:26<00:23, 57112.90it/s]\n",
      "example_01b_base 2019-01-16 20:00:00:  51%|█████     | 1371601.0/2678400 [00:26<00:22, 58512.99it/s]\n",
      "example_01b_base 2019-01-16 22:00:00:  51%|█████▏    | 1378801.0/2678400 [00:26<00:27, 48008.13it/s]\n",
      "example_01b_base 2019-01-17 00:00:00:  52%|█████▏    | 1386001.0/2678400 [00:26<00:25, 51141.69it/s]\n",
      "example_01b_base 2019-01-17 02:00:00:  52%|█████▏    | 1393201.0/2678400 [00:26<00:24, 53313.66it/s]\n",
      "example_01b_base 2019-01-17 04:00:00:  52%|█████▏    | 1400401.0/2678400 [00:26<00:23, 53753.71it/s]\n",
      "example_01b_base 2019-01-17 06:00:00:  53%|█████▎    | 1407601.0/2678400 [00:27<00:23, 54586.25it/s]\n",
      "example_01b_base 2019-01-17 08:00:00:  53%|█████▎    | 1414801.0/2678400 [00:27<00:22, 55143.83it/s]\n",
      "example_01b_base 2019-01-17 10:00:00:  53%|█████▎    | 1422001.0/2678400 [00:27<00:22, 56438.37it/s]\n",
      "example_01b_base 2019-01-17 12:00:00:  53%|█████▎    | 1429201.0/2678400 [00:27<00:27, 46130.36it/s]\n",
      "example_01b_base 2019-01-17 14:00:00:  54%|█████▎    | 1436401.0/2678400 [00:27<00:25, 48948.75it/s]\n",
      "example_01b_base 2019-01-17 16:00:00:  54%|█████▍    | 1443601.0/2678400 [00:27<00:23, 51539.08it/s]\n",
      "example_01b_base 2019-01-17 18:00:00:  54%|█████▍    | 1450801.0/2678400 [00:27<00:23, 52826.92it/s]\n",
      "example_01b_base 2019-01-17 20:00:00:  54%|█████▍    | 1458001.0/2678400 [00:28<00:23, 51758.52it/s]\n",
      "example_01b_base 2019-01-17 22:00:00:  55%|█████▍    | 1465201.0/2678400 [00:28<00:28, 42717.40it/s]\n",
      "example_01b_base 2019-01-18 00:00:00:  55%|█████▍    | 1472401.0/2678400 [00:28<00:26, 44945.04it/s]\n",
      "example_01b_base 2019-01-18 02:00:00:  55%|█████▌    | 1479601.0/2678400 [00:28<00:25, 47894.34it/s]\n",
      "example_01b_base 2019-01-18 04:00:00:  56%|█████▌    | 1486801.0/2678400 [00:28<00:23, 50365.82it/s]\n",
      "example_01b_base 2019-01-18 06:00:00:  56%|█████▌    | 1494001.0/2678400 [00:28<00:22, 52019.87it/s]\n",
      "example_01b_base 2019-01-18 08:00:00:  56%|█████▌    | 1501201.0/2678400 [00:28<00:22, 53161.15it/s]\n",
      "example_01b_base 2019-01-18 10:00:00:  56%|█████▋    | 1508401.0/2678400 [00:29<00:21, 54133.32it/s]\n",
      "example_01b_base 2019-01-18 12:00:00:  57%|█████▋    | 1515601.0/2678400 [00:29<00:21, 54511.42it/s]\n",
      "example_01b_base 2019-01-18 14:00:00:  57%|█████▋    | 1522801.0/2678400 [00:29<00:21, 54891.42it/s]\n",
      "example_01b_base 2019-01-18 16:00:00:  57%|█████▋    | 1530001.0/2678400 [00:29<00:20, 55329.28it/s]\n",
      "example_01b_base 2019-01-18 18:00:00:  57%|█████▋    | 1537201.0/2678400 [00:29<00:20, 55896.99it/s]\n",
      "example_01b_base 2019-01-18 20:00:00:  58%|█████▊    | 1544401.0/2678400 [00:29<00:20, 56313.74it/s]\n",
      "example_01b_base 2019-01-18 22:00:00:  58%|█████▊    | 1551601.0/2678400 [00:29<00:23, 47523.94it/s]\n",
      "example_01b_base 2019-01-19 00:00:00:  58%|█████▊    | 1558801.0/2678400 [00:30<00:22, 50134.62it/s]\n",
      "example_01b_base 2019-01-19 02:00:00:  58%|█████▊    | 1566001.0/2678400 [00:30<00:21, 52135.92it/s]\n",
      "example_01b_base 2019-01-19 04:00:00:  59%|█████▊    | 1573201.0/2678400 [00:30<00:20, 53633.37it/s]\n",
      "example_01b_base 2019-01-19 06:00:00:  59%|█████▉    | 1580401.0/2678400 [00:30<00:20, 54759.66it/s]\n",
      "example_01b_base 2019-01-19 08:00:00:  59%|█████▉    | 1587601.0/2678400 [00:30<00:19, 55532.05it/s]\n",
      "example_01b_base 2019-01-19 10:00:00:  60%|█████▉    | 1594801.0/2678400 [00:30<00:19, 55818.85it/s]\n",
      "example_01b_base 2019-01-19 12:00:00:  60%|█████▉    | 1602001.0/2678400 [00:30<00:19, 56034.64it/s]\n",
      "example_01b_base 2019-01-19 14:00:00:  60%|██████    | 1609201.0/2678400 [00:30<00:18, 56601.15it/s]\n",
      "example_01b_base 2019-01-19 16:00:00:  60%|██████    | 1616401.0/2678400 [00:31<00:18, 57218.03it/s]\n",
      "example_01b_base 2019-01-19 18:00:00:  61%|██████    | 1623601.0/2678400 [00:31<00:18, 57910.27it/s]\n",
      "example_01b_base 2019-01-19 20:00:00:  61%|██████    | 1630801.0/2678400 [00:31<00:17, 58361.48it/s]\n",
      "example_01b_base 2019-01-19 22:00:00:  61%|██████    | 1638001.0/2678400 [00:31<00:25, 40438.40it/s]\n",
      "example_01b_base 2019-01-20 00:00:00:  61%|██████▏   | 1645201.0/2678400 [00:31<00:23, 44806.81it/s]\n",
      "example_01b_base 2019-01-20 02:00:00:  62%|██████▏   | 1652401.0/2678400 [00:31<00:21, 48232.89it/s]\n",
      "example_01b_base 2019-01-20 04:00:00:  62%|██████▏   | 1659601.0/2678400 [00:32<00:19, 50942.26it/s]\n",
      "example_01b_base 2019-01-20 06:00:00:  62%|██████▏   | 1666801.0/2678400 [00:32<00:19, 52577.28it/s]\n",
      "example_01b_base 2019-01-20 08:00:00:  63%|██████▎   | 1674001.0/2678400 [00:32<00:18, 54066.08it/s]\n",
      "example_01b_base 2019-01-20 10:00:00:  63%|██████▎   | 1681201.0/2678400 [00:32<00:18, 54953.43it/s]\n",
      "example_01b_base 2019-01-20 12:00:00:  63%|██████▎   | 1688401.0/2678400 [00:32<00:17, 55782.80it/s]\n",
      "example_01b_base 2019-01-20 14:00:00:  63%|██████▎   | 1695601.0/2678400 [00:32<00:17, 56177.74it/s]\n",
      "example_01b_base 2019-01-20 16:00:00:  64%|██████▎   | 1702801.0/2678400 [00:32<00:17, 56493.60it/s]\n",
      "example_01b_base 2019-01-20 18:00:00:  64%|██████▍   | 1710001.0/2678400 [00:32<00:17, 56898.11it/s]\n",
      "example_01b_base 2019-01-20 20:00:00:  64%|██████▍   | 1717201.0/2678400 [00:32<00:16, 57571.14it/s]\n",
      "example_01b_base 2019-01-20 22:00:00:  64%|██████▍   | 1724401.0/2678400 [00:33<00:20, 46939.35it/s]\n",
      "example_01b_base 2019-01-21 00:00:00:  65%|██████▍   | 1731601.0/2678400 [00:33<00:20, 46948.39it/s]\n",
      "example_01b_base 2019-01-21 02:00:00:  65%|██████▍   | 1738801.0/2678400 [00:33<00:19, 47157.15it/s]\n",
      "example_01b_base 2019-01-21 04:00:00:  65%|██████▌   | 1746001.0/2678400 [00:33<00:19, 47640.75it/s]\n",
      "example_01b_base 2019-01-21 06:00:00:  65%|██████▌   | 1753201.0/2678400 [00:33<00:18, 50345.13it/s]\n",
      "example_01b_base 2019-01-21 08:00:00:  66%|██████▌   | 1760401.0/2678400 [00:33<00:17, 52500.44it/s]\n",
      "example_01b_base 2019-01-21 10:00:00:  66%|██████▌   | 1767601.0/2678400 [00:34<00:16, 53850.08it/s]\n",
      "example_01b_base 2019-01-21 12:00:00:  66%|██████▋   | 1774801.0/2678400 [00:34<00:16, 54909.79it/s]\n",
      "example_01b_base 2019-01-21 14:00:00:  67%|██████▋   | 1782001.0/2678400 [00:34<00:16, 55427.82it/s]\n",
      "example_01b_base 2019-01-21 16:00:00:  67%|██████▋   | 1789201.0/2678400 [00:34<00:15, 55772.03it/s]\n",
      "example_01b_base 2019-01-21 18:00:00:  67%|██████▋   | 1796401.0/2678400 [00:34<00:15, 56081.53it/s]\n",
      "example_01b_base 2019-01-21 20:00:00:  67%|██████▋   | 1803601.0/2678400 [00:34<00:15, 56167.70it/s]\n",
      "example_01b_base 2019-01-21 22:00:00:  68%|██████▊   | 1810801.0/2678400 [00:34<00:18, 47191.56it/s]\n",
      "example_01b_base 2019-01-22 00:00:00:  68%|██████▊   | 1818001.0/2678400 [00:35<00:17, 49666.27it/s]\n",
      "example_01b_base 2019-01-22 02:00:00:  68%|██████▊   | 1825201.0/2678400 [00:35<00:19, 43630.25it/s]\n",
      "example_01b_base 2019-01-22 04:00:00:  68%|██████▊   | 1832401.0/2678400 [00:35<00:17, 47314.40it/s]\n",
      "example_01b_base 2019-01-22 06:00:00:  69%|██████▊   | 1839601.0/2678400 [00:35<00:16, 49706.53it/s]\n",
      "example_01b_base 2019-01-22 08:00:00:  69%|██████▉   | 1846801.0/2678400 [00:35<00:16, 51315.03it/s]\n",
      "example_01b_base 2019-01-22 10:00:00:  69%|██████▉   | 1854001.0/2678400 [00:35<00:15, 52804.29it/s]\n",
      "example_01b_base 2019-01-22 12:00:00:  69%|██████▉   | 1861201.0/2678400 [00:35<00:15, 54172.45it/s]\n",
      "example_01b_base 2019-01-22 14:00:00:  70%|██████▉   | 1868401.0/2678400 [00:35<00:14, 55039.31it/s]\n",
      "example_01b_base 2019-01-22 16:00:00:  70%|███████   | 1875601.0/2678400 [00:36<00:14, 55851.32it/s]\n",
      "example_01b_base 2019-01-22 18:00:00:  70%|███████   | 1882801.0/2678400 [00:36<00:14, 56532.03it/s]\n",
      "example_01b_base 2019-01-22 20:00:00:  71%|███████   | 1890001.0/2678400 [00:36<00:13, 57150.13it/s]\n",
      "example_01b_base 2019-01-22 22:00:00:  71%|███████   | 1897201.0/2678400 [00:36<00:16, 47887.19it/s]\n",
      "example_01b_base 2019-01-23 00:00:00:  71%|███████   | 1904401.0/2678400 [00:36<00:15, 50207.74it/s]\n",
      "example_01b_base 2019-01-23 02:00:00:  71%|███████▏  | 1911601.0/2678400 [00:36<00:14, 52304.69it/s]\n",
      "example_01b_base 2019-01-23 04:00:00:  72%|███████▏  | 1918801.0/2678400 [00:36<00:14, 53701.26it/s]\n",
      "example_01b_base 2019-01-23 06:00:00:  72%|███████▏  | 1926001.0/2678400 [00:37<00:13, 54602.21it/s]\n",
      "example_01b_base 2019-01-23 08:00:00:  72%|███████▏  | 1933201.0/2678400 [00:37<00:13, 55284.60it/s]\n",
      "example_01b_base 2019-01-23 10:00:00:  72%|███████▏  | 1940401.0/2678400 [00:37<00:13, 55723.14it/s]\n",
      "example_01b_base 2019-01-23 12:00:00:  73%|███████▎  | 1947601.0/2678400 [00:37<00:12, 56417.24it/s]\n",
      "example_01b_base 2019-01-23 14:00:00:  73%|███████▎  | 1954801.0/2678400 [00:37<00:12, 56766.80it/s]\n",
      "example_01b_base 2019-01-23 16:00:00:  73%|███████▎  | 1962001.0/2678400 [00:37<00:12, 56570.68it/s]\n",
      "example_01b_base 2019-01-23 18:00:00:  74%|███████▎  | 1969201.0/2678400 [00:37<00:12, 56681.55it/s]\n",
      "example_01b_base 2019-01-23 20:00:00:  74%|███████▍  | 1976401.0/2678400 [00:37<00:12, 56994.01it/s]\n",
      "example_01b_base 2019-01-23 22:00:00:  74%|███████▍  | 1983601.0/2678400 [00:38<00:14, 47395.68it/s]\n",
      "example_01b_base 2019-01-24 00:00:00:  74%|███████▍  | 1990801.0/2678400 [00:38<00:13, 49827.36it/s]\n",
      "example_01b_base 2019-01-24 02:00:00:  75%|███████▍  | 1998001.0/2678400 [00:38<00:13, 50530.95it/s]\n",
      "example_01b_base 2019-01-24 04:00:00:  75%|███████▍  | 2005201.0/2678400 [00:38<00:13, 48896.84it/s]\n",
      "example_01b_base 2019-01-24 06:00:00:  75%|███████▌  | 2012401.0/2678400 [00:38<00:13, 48300.49it/s]\n",
      "example_01b_base 2019-01-24 08:00:00:  75%|███████▌  | 2019601.0/2678400 [00:38<00:13, 48353.84it/s]\n",
      "example_01b_base 2019-01-24 10:00:00:  76%|███████▌  | 2026801.0/2678400 [00:39<00:12, 50454.86it/s]\n",
      "example_01b_base 2019-01-24 12:00:00:  76%|███████▌  | 2034001.0/2678400 [00:39<00:15, 42916.64it/s]\n",
      "example_01b_base 2019-01-24 14:00:00:  76%|███████▌  | 2041201.0/2678400 [00:39<00:13, 46473.77it/s]\n",
      "example_01b_base 2019-01-24 16:00:00:  76%|███████▋  | 2048401.0/2678400 [00:39<00:12, 49259.40it/s]\n",
      "example_01b_base 2019-01-24 18:00:00:  77%|███████▋  | 2055601.0/2678400 [00:39<00:12, 51471.87it/s]\n",
      "example_01b_base 2019-01-24 20:00:00:  77%|███████▋  | 2062801.0/2678400 [00:39<00:11, 52871.89it/s]\n",
      "example_01b_base 2019-01-24 22:00:00:  77%|███████▋  | 2070001.0/2678400 [00:39<00:13, 44850.86it/s]\n",
      "example_01b_base 2019-01-25 00:00:00:  78%|███████▊  | 2077201.0/2678400 [00:40<00:12, 47972.90it/s]\n",
      "example_01b_base 2019-01-25 02:00:00:  78%|███████▊  | 2084401.0/2678400 [00:40<00:11, 50364.92it/s]\n",
      "example_01b_base 2019-01-25 04:00:00:  78%|███████▊  | 2091601.0/2678400 [00:40<00:11, 52129.92it/s]\n",
      "example_01b_base 2019-01-25 06:00:00:  78%|███████▊  | 2098801.0/2678400 [00:40<00:10, 53099.47it/s]\n",
      "example_01b_base 2019-01-25 08:00:00:  79%|███████▊  | 2106001.0/2678400 [00:40<00:10, 53979.10it/s]\n",
      "example_01b_base 2019-01-25 10:00:00:  79%|███████▉  | 2113201.0/2678400 [00:40<00:10, 54551.34it/s]\n",
      "example_01b_base 2019-01-25 12:00:00:  79%|███████▉  | 2120401.0/2678400 [00:40<00:10, 54708.10it/s]\n",
      "example_01b_base 2019-01-25 14:00:00:  79%|███████▉  | 2127601.0/2678400 [00:40<00:09, 55326.01it/s]\n",
      "example_01b_base 2019-01-25 16:00:00:  80%|███████▉  | 2134801.0/2678400 [00:41<00:09, 55315.55it/s]\n",
      "example_01b_base 2019-01-25 18:00:00:  80%|███████▉  | 2142001.0/2678400 [00:41<00:09, 55719.57it/s]\n",
      "example_01b_base 2019-01-25 20:00:00:  80%|████████  | 2149201.0/2678400 [00:41<00:09, 56108.37it/s]\n",
      "example_01b_base 2019-01-25 22:00:00:  81%|████████  | 2156401.0/2678400 [00:41<00:10, 48068.49it/s]\n",
      "example_01b_base 2019-01-26 00:00:00:  81%|████████  | 2163601.0/2678400 [00:41<00:10, 50651.70it/s]\n",
      "example_01b_base 2019-01-26 02:00:00:  81%|████████  | 2170801.0/2678400 [00:41<00:09, 52754.23it/s]\n",
      "example_01b_base 2019-01-26 04:00:00:  81%|████████▏ | 2178001.0/2678400 [00:41<00:09, 54401.12it/s]\n",
      "example_01b_base 2019-01-26 06:00:00:  82%|████████▏ | 2185201.0/2678400 [00:42<00:08, 55573.23it/s]\n",
      "example_01b_base 2019-01-26 08:00:00:  82%|████████▏ | 2192401.0/2678400 [00:42<00:08, 55938.28it/s]\n",
      "example_01b_base 2019-01-26 10:00:00:  82%|████████▏ | 2199601.0/2678400 [00:42<00:08, 56163.10it/s]\n",
      "example_01b_base 2019-01-26 12:00:00:  82%|████████▏ | 2206801.0/2678400 [00:42<00:08, 56101.46it/s]\n",
      "example_01b_base 2019-01-26 14:00:00:  83%|████████▎ | 2214001.0/2678400 [00:42<00:08, 56020.08it/s]\n",
      "example_01b_base 2019-01-26 16:00:00:  83%|████████▎ | 2221201.0/2678400 [00:42<00:08, 55954.57it/s]\n",
      "example_01b_base 2019-01-26 18:00:00:  83%|████████▎ | 2228401.0/2678400 [00:42<00:08, 56247.45it/s]\n",
      "example_01b_base 2019-01-26 20:00:00:  83%|████████▎ | 2235601.0/2678400 [00:42<00:07, 56625.95it/s]\n",
      "example_01b_base 2019-01-26 22:00:00:  84%|████████▎ | 2242801.0/2678400 [00:43<00:10, 39749.59it/s]\n",
      "example_01b_base 2019-01-27 00:00:00:  84%|████████▍ | 2250001.0/2678400 [00:43<00:09, 44083.38it/s]\n",
      "example_01b_base 2019-01-27 02:00:00:  84%|████████▍ | 2257201.0/2678400 [00:43<00:08, 47261.52it/s]\n",
      "example_01b_base 2019-01-27 04:00:00:  85%|████████▍ | 2264401.0/2678400 [00:43<00:09, 45065.33it/s]\n",
      "example_01b_base 2019-01-27 06:00:00:  85%|████████▍ | 2271601.0/2678400 [00:43<00:09, 43532.27it/s]\n",
      "example_01b_base 2019-01-27 08:00:00:  85%|████████▌ | 2278801.0/2678400 [00:44<00:09, 44365.05it/s]\n",
      "example_01b_base 2019-01-27 10:00:00:  85%|████████▌ | 2286001.0/2678400 [00:44<00:08, 46235.03it/s]\n",
      "example_01b_base 2019-01-27 12:00:00:  86%|████████▌ | 2293201.0/2678400 [00:44<00:07, 48999.17it/s]\n",
      "example_01b_base 2019-01-27 14:00:00:  86%|████████▌ | 2300401.0/2678400 [00:44<00:07, 50851.62it/s]\n",
      "example_01b_base 2019-01-27 16:00:00:  86%|████████▌ | 2307601.0/2678400 [00:44<00:07, 52206.04it/s]\n",
      "example_01b_base 2019-01-27 18:00:00:  86%|████████▋ | 2314801.0/2678400 [00:44<00:06, 53737.54it/s]\n",
      "example_01b_base 2019-01-27 20:00:00:  87%|████████▋ | 2322001.0/2678400 [00:44<00:06, 54679.55it/s]\n",
      "example_01b_base 2019-01-27 22:00:00:  87%|████████▋ | 2329201.0/2678400 [00:45<00:07, 46289.56it/s]\n",
      "example_01b_base 2019-01-28 00:00:00:  87%|████████▋ | 2336401.0/2678400 [00:45<00:06, 48965.59it/s]\n",
      "example_01b_base 2019-01-28 02:00:00:  88%|████████▊ | 2343601.0/2678400 [00:45<00:06, 51236.44it/s]\n",
      "example_01b_base 2019-01-28 04:00:00:  88%|████████▊ | 2350801.0/2678400 [00:45<00:06, 52810.02it/s]\n",
      "example_01b_base 2019-01-28 06:00:00:  88%|████████▊ | 2358001.0/2678400 [00:45<00:05, 53571.46it/s]\n",
      "example_01b_base 2019-01-28 08:00:00:  88%|████████▊ | 2365201.0/2678400 [00:45<00:05, 54102.86it/s]\n",
      "example_01b_base 2019-01-28 10:00:00:  89%|████████▊ | 2372401.0/2678400 [00:45<00:05, 54346.51it/s]\n",
      "example_01b_base 2019-01-28 12:00:00:  89%|████████▉ | 2379601.0/2678400 [00:45<00:05, 54802.59it/s]\n",
      "example_01b_base 2019-01-28 14:00:00:  89%|████████▉ | 2386801.0/2678400 [00:46<00:05, 55136.18it/s]\n",
      "example_01b_base 2019-01-28 16:00:00:  89%|████████▉ | 2394001.0/2678400 [00:46<00:05, 55364.17it/s]\n",
      "example_01b_base 2019-01-28 18:00:00:  90%|████████▉ | 2401201.0/2678400 [00:46<00:04, 55746.99it/s]\n",
      "example_01b_base 2019-01-28 20:00:00:  90%|████████▉ | 2408401.0/2678400 [00:46<00:04, 56315.38it/s]\n",
      "example_01b_base 2019-01-28 22:00:00:  90%|█████████ | 2415601.0/2678400 [00:46<00:05, 47236.82it/s]\n",
      "example_01b_base 2019-01-29 00:00:00:  90%|█████████ | 2422801.0/2678400 [00:46<00:05, 49753.98it/s]\n",
      "example_01b_base 2019-01-29 02:00:00:  91%|█████████ | 2430001.0/2678400 [00:46<00:04, 51792.22it/s]\n",
      "example_01b_base 2019-01-29 04:00:00:  91%|█████████ | 2437201.0/2678400 [00:47<00:04, 53205.46it/s]\n",
      "example_01b_base 2019-01-29 06:00:00:  91%|█████████▏| 2444401.0/2678400 [00:47<00:04, 53969.83it/s]\n",
      "example_01b_base 2019-01-29 08:00:00:  92%|█████████▏| 2451601.0/2678400 [00:47<00:05, 44815.15it/s]\n",
      "example_01b_base 2019-01-29 10:00:00:  92%|█████████▏| 2458801.0/2678400 [00:47<00:04, 47933.80it/s]\n",
      "example_01b_base 2019-01-29 12:00:00:  92%|█████████▏| 2466001.0/2678400 [00:47<00:04, 50547.87it/s]\n",
      "example_01b_base 2019-01-29 14:00:00:  92%|█████████▏| 2473201.0/2678400 [00:47<00:03, 51979.14it/s]\n",
      "example_01b_base 2019-01-29 16:00:00:  93%|█████████▎| 2480401.0/2678400 [00:47<00:03, 53483.95it/s]\n",
      "example_01b_base 2019-01-29 18:00:00:  93%|█████████▎| 2487601.0/2678400 [00:47<00:03, 55132.95it/s]\n",
      "example_01b_base 2019-01-29 20:00:00:  93%|█████████▎| 2494801.0/2678400 [00:48<00:03, 56075.62it/s]\n",
      "example_01b_base 2019-01-29 22:00:00:  93%|█████████▎| 2502001.0/2678400 [00:48<00:03, 47078.67it/s]\n",
      "example_01b_base 2019-01-30 00:00:00:  94%|█████████▎| 2509201.0/2678400 [00:48<00:03, 49527.95it/s]\n",
      "example_01b_base 2019-01-30 02:00:00:  94%|█████████▍| 2516401.0/2678400 [00:48<00:03, 51567.59it/s]\n",
      "example_01b_base 2019-01-30 04:00:00:  94%|█████████▍| 2523601.0/2678400 [00:48<00:02, 53024.99it/s]\n",
      "example_01b_base 2019-01-30 06:00:00:  94%|█████████▍| 2530801.0/2678400 [00:48<00:02, 52672.69it/s]\n",
      "example_01b_base 2019-01-30 08:00:00:  95%|█████████▍| 2538001.0/2678400 [00:48<00:02, 50730.29it/s]\n",
      "example_01b_base 2019-01-30 10:00:00:  95%|█████████▌| 2545201.0/2678400 [00:49<00:02, 49515.74it/s]\n",
      "example_01b_base 2019-01-30 12:00:00:  95%|█████████▌| 2552401.0/2678400 [00:49<00:02, 48620.15it/s]\n",
      "example_01b_base 2019-01-30 14:00:00:  96%|█████████▌| 2559601.0/2678400 [00:49<00:02, 50718.42it/s]\n",
      "example_01b_base 2019-01-30 16:00:00:  96%|█████████▌| 2566801.0/2678400 [00:49<00:02, 52184.63it/s]\n",
      "example_01b_base 2019-01-30 18:00:00:  96%|█████████▌| 2574001.0/2678400 [00:49<00:01, 53257.21it/s]\n",
      "example_01b_base 2019-01-30 20:00:00:  96%|█████████▋| 2581201.0/2678400 [00:49<00:01, 54722.46it/s]\n",
      "example_01b_base 2019-01-30 22:00:00:  97%|█████████▋| 2588401.0/2678400 [00:50<00:01, 47263.58it/s]\n",
      "example_01b_base 2019-01-31 00:00:00:  97%|█████████▋| 2595601.0/2678400 [00:50<00:01, 50040.08it/s]\n",
      "example_01b_base 2019-01-31 02:00:00:  97%|█████████▋| 2602801.0/2678400 [00:50<00:01, 52328.95it/s]\n",
      "example_01b_base 2019-01-31 04:00:00:  97%|█████████▋| 2610001.0/2678400 [00:50<00:01, 53431.38it/s]\n",
      "example_01b_base 2019-01-31 06:00:00:  98%|█████████▊| 2617201.0/2678400 [00:50<00:01, 54276.74it/s]\n",
      "example_01b_base 2019-01-31 08:00:00:  98%|█████████▊| 2624401.0/2678400 [00:50<00:00, 55024.43it/s]\n",
      "example_01b_base 2019-01-31 10:00:00:  98%|█████████▊| 2631601.0/2678400 [00:50<00:00, 55595.47it/s]\n",
      "example_01b_base 2019-01-31 12:00:00:  99%|█████████▊| 2638801.0/2678400 [00:50<00:00, 56003.42it/s]\n",
      "example_01b_base 2019-01-31 14:00:00:  99%|█████████▉| 2646001.0/2678400 [00:51<00:00, 56331.64it/s]\n",
      "example_01b_base 2019-01-31 16:00:00:  99%|█████████▉| 2653201.0/2678400 [00:51<00:00, 56264.82it/s]\n",
      "example_01b_base 2019-01-31 18:00:00:  99%|█████████▉| 2660401.0/2678400 [00:51<00:00, 45663.20it/s]\n",
      "example_01b_base 2019-01-31 20:00:00: 100%|█████████▉| 2667601.0/2678400 [00:51<00:00, 49173.80it/s]\n",
      "example_01b_base 2019-01-31 22:00:00: 100%|█████████▉| 2674801.0/2678400 [00:51<00:00, 43804.79it/s]\n",
      "example_01b_base 2019-02-01 00:00:00: : 2678401.0it [00:51, 51768.52it/s]                           \n"
     ]
    }
   ],
   "source": [
    "!cd assume && assume -s example_01b -db \"sqlite:///./examples/local_db/assume_db_example_01b.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj2C4ElILNNv"
   },
   "source": [
    "## 1. From one simulation year to learning episodes\n",
    "\n",
    "In a normal simulation wihtout reinforcement learning, we only run the time horizon of the simulation once. For RL the agents need to learn their strategy based on interactions. For that to work an RL agent has to see a situation, aka a simulation hour, multiple times, and hence we need to run the entire silumation hoirzon multiple times as well.   \n",
    "\n",
    "To enable this we define a run learning function that will be called if the simulation is started and we defined in our config that we want to activate learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMyZhaNM7NRP"
   },
   "source": [
    "**But first some imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xUsbeZdPJ_2Q"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import yaml\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from assume.common.exceptions import AssumeException\n",
    "from assume.reinforcement_learning.algorithms.base_algorithm import RLAlgorithm\n",
    "from assume.reinforcement_learning.algorithms.matd3 import TD3\n",
    "from assume.reinforcement_learning.buffer import ReplayBuffer\n",
    "from assume.reinforcement_learning.learning_role import Learning\n",
    "from assume.reinforcement_learning.learning_utils import polyak_update\n",
    "from assume.scenario.loader_csv import (\n",
    "    load_config_and_create_forecaster,\n",
    "    load_scenario_folder,\n",
    "    setup_world,\n",
    ")\n",
    "from assume.world import World\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This flowchart provides an overview of the key stages involved in the run_learning function, which trains Deep Reinforcement Learning (DRL) agents within a simulated market environment. The process is divided into five main steps:\n",
    "\n",
    "**Initialization of the Learning Process**: The function begins by setting up the environment, initializing policies, and configuring necessary settings such as logging and buffer allocation. It ensures that no existing policies are overwritten without confirmation.\n",
    "\n",
    "**Training Loop**: This is the outer loop where multiple training episodes are executed. For each episode, the world simulation is completely re-initialized and reset after execution, meaning the simulation environment is essentially killed after each episode. Crucially, all necessary information that must persist across episodes—such as collected experience stored in the buffer—is maintained in the inter-episodic data. This data is key to ensuring the continuity of the learning process as it allows the DRL agents to build knowledge over time.\n",
    "\n",
    "**Evaluation Loop**: Nested within the training loop, the evaluation loop periodically assesses the performance of the learned policies. Based on average rewards, the best-performing policies are saved, and the function determines if further training is necessary.\n",
    "\n",
    "**Terminate Learning and Save Policies**: At the end of the training phase, the function saves the final version of the learned policies, ensuring that the results are stored for future use.\n",
    "\n",
    "**Final Evaluation Run**: A final evaluation run is conducted using the best policies from the training phase, providing a benchmark for overall performance.\n",
    "\n",
    "The flowchart visually represents the interaction between the training and evaluation loops, highlighting the progression through these key stages.\n",
    "\n",
    "<img src=\"../../docs/source/img/Assume_run_learning_loop.png\" alt=\"Learning Process Flowchart\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UXYSesx4Ifp5"
   },
   "outputs": [],
   "source": [
    "def run_learning(\n",
    "    world: World,\n",
    "    inputs_path: str,\n",
    "    scenario: str,\n",
    "    study_case: str,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train Deep Reinforcement Learning (DRL) agents to act in a simulated market environment.\n",
    "\n",
    "    This function runs multiple episodes of simulation to train DRL agents, performs evaluation, and saves the best runs. It maintains the buffer and learned agents in memory to avoid resetting them with each new run.\n",
    "\n",
    "    Args:\n",
    "        world (World): An instance of the World class representing the simulation environment.\n",
    "        inputs_path (str): The path to the folder containing input files necessary for the simulation.\n",
    "        scenario (str): The name of the scenario for the simulation.\n",
    "        study_case (str): The specific study case for the simulation.\n",
    "\n",
    "    Note:\n",
    "        - The function uses a ReplayBuffer to store experiences for training the DRL agents.\n",
    "        - It iterates through training episodes, updating the agents and evaluating their performance at regular intervals.\n",
    "        - Initial exploration is active at the beginning and is disabled after a certain number of episodes to improve the performance of DRL algorithms.\n",
    "        - Upon completion of training, the function performs an evaluation run using the best policy learned during training.\n",
    "        - The best policies are chosen based on the average reward obtained during the evaluation runs, and they are saved for future use.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1 - Initialisation of the learning process\n",
    "\n",
    "    if not verbose:\n",
    "        logger.setLevel(logging.WARNING)\n",
    "\n",
    "    # remove csv path so that nothing is written while learning\n",
    "    temp_csv_path = world.export_csv_path\n",
    "    world.export_csv_path = \"\"\n",
    "\n",
    "    # initialize policies already here to set the obs_dim and act_dim in the learning role\n",
    "    actors_and_critics = None\n",
    "    world.learning_role.initialize_policy(actors_and_critics=actors_and_critics)\n",
    "    world.output_role.del_similar_runs()\n",
    "\n",
    "    # check if we already stored policies for this simualtion\n",
    "    save_path = world.learning_config[\"trained_policies_save_path\"]\n",
    "\n",
    "    if Path(save_path).is_dir():\n",
    "        # we are in learning mode and about to train new policies, which might overwrite existing ones\n",
    "        accept = input(\n",
    "            f\"{save_path=} exists - should we overwrite current learnings? (y/N) \"\n",
    "        )\n",
    "        if not accept.lower().startswith(\"y\"):\n",
    "            # stop here - do not start learning or save anything\n",
    "            raise AssumeException(\"don't overwrite existing strategies\")\n",
    "\n",
    "    # Load scenario data to reuse across episodes\n",
    "    scenario_data = load_config_and_create_forecaster(inputs_path, scenario, study_case)\n",
    "\n",
    "    # Information that needs to be stored across episodes, aka one simulation run\n",
    "    inter_episodic_data = {\n",
    "        \"buffer\": ReplayBuffer(\n",
    "            buffer_size=int(world.learning_config.get(\"replay_buffer_size\", 5e5)),\n",
    "            obs_dim=world.learning_role.rl_algorithm.obs_dim,\n",
    "            act_dim=world.learning_role.rl_algorithm.act_dim,\n",
    "            n_rl_units=len(world.learning_role.rl_strats),\n",
    "            device=world.learning_role.device,\n",
    "            float_type=world.learning_role.float_type,\n",
    "        ),\n",
    "        \"actors_and_critics\": None,\n",
    "        \"max_eval\": defaultdict(lambda: -1e9),\n",
    "        \"all_eval\": defaultdict(list),\n",
    "        \"avg_all_eval\": [],\n",
    "        \"episodes_done\": 0,\n",
    "        \"eval_episodes_done\": 0,\n",
    "        \"noise_scale\": world.learning_config.get(\"noise_scale\", 1.0),\n",
    "    }\n",
    "\n",
    "    validation_interval = min(\n",
    "        world.learning_role.training_episodes,\n",
    "        world.learning_config.get(\"validation_episodes_interval\", 5),\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 2 - Training loop\n",
    "\n",
    "    eval_episode = 1\n",
    "\n",
    "    for episode in tqdm(\n",
    "        range(1, world.learning_role.training_episodes + 1),\n",
    "        desc=\"Training Episodes\",\n",
    "    ):\n",
    "        # TODO normally, loading twice should not create issues, somehow a scheduling issue is raised currently\n",
    "        if episode != 1:\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                scenario_data=scenario_data,\n",
    "                study_case=study_case,\n",
    "                episode=episode,\n",
    "            )\n",
    "\n",
    "        # Give the newly initliazed learning role the needed information across episodes\n",
    "        world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "        world.run()\n",
    "\n",
    "        # Store updated information across episodes\n",
    "        inter_episodic_data = world.learning_role.get_inter_episodic_data()\n",
    "        inter_episodic_data[\"episodes_done\"] = episode\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 3 - Evaluation loop\n",
    "\n",
    "        if (\n",
    "            episode % validation_interval == 0\n",
    "            and episode\n",
    "            >= world.learning_role.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.reset()\n",
    "\n",
    "            # load evaluation run\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                scenario_data=scenario_data,\n",
    "                study_case=study_case,\n",
    "                perform_evaluation=True,\n",
    "                eval_episode=eval_episode,\n",
    "            )\n",
    "\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "            world.run()\n",
    "\n",
    "            total_rewards = world.output_role.get_sum_reward()\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            # check reward improvement in evaluation run\n",
    "            # and store best run in eval folder\n",
    "            terminate = world.learning_role.compare_and_save_policies(\n",
    "                {\"avg_reward\": avg_reward}\n",
    "            )\n",
    "\n",
    "            inter_episodic_data[\"eval_episodes_done\"] = eval_episode\n",
    "\n",
    "            # if we have not improved in the last x evaluations, we stop loop\n",
    "            if terminate:\n",
    "                break\n",
    "\n",
    "            eval_episode += 1\n",
    "\n",
    "        world.reset()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 4 - Terminate Learning and Save policies\n",
    "\n",
    "        # if at end of simulation save last policies\n",
    "        if episode == (world.learning_role.training_episodes):\n",
    "            world.learning_role.rl_algorithm.save_params(\n",
    "                directory=f\"{world.learning_role.trained_policies_save_path}/last_policies\"\n",
    "            )\n",
    "\n",
    "        # container shutdown implicitly with new initialisation\n",
    "    logger.info(\"################\")\n",
    "    logger.info(\"Training finished, Start evaluation run\")\n",
    "    world.export_csv_path = temp_csv_path\n",
    "\n",
    "    world.reset()\n",
    "\n",
    "    # ----------------------------------\n",
    "    # 5 - Final Evaluation run\n",
    "\n",
    "    # load scenario for evaluation\n",
    "    setup_world(\n",
    "        world=world,\n",
    "        scenario_data=scenario_data,\n",
    "        study_case=study_case,\n",
    "        terminate_learning=True,\n",
    "    )\n",
    "\n",
    "    world.learning_role.load_inter_episodic_data(inter_episodic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UM1QPZrIdqK"
   },
   "source": [
    "## 2. What role has a learning role\n",
    "\n",
    "The LearningRole class in learning_role.py is a central component of the reinforcement learning framework. It manages configurations, device settings, early stopping of the learning process, and initializes various RL strategies the algorithm and buffers. This class ensures that the RL agent can be trained or evaluated effectively, leveraging the available hardware and adhering to the specified configurations. The parameters of the learning process are also described in the read-the-docs under learning_algorithms.\n",
    "\n",
    "### 2.1 Learning Data Management\n",
    "\n",
    "One key feature of the LearningRole class is its ability to load and manage the inter episodic data. This involves storing experiences and the training progress and retrieving this data to train the RL agent. By efficiently handling episodic data, the LearningRole class enables the agent to learn from past experiences and improve its performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learning(Learning):\n",
    "    \"\"\"\n",
    "    This class manages the learning process of reinforcement learning agents, including initializing key components such as\n",
    "    neural networks, replay buffer, and learning hyperparameters. It handles both training and evaluation modes based on\n",
    "    the provided learning configuration.\n",
    "\n",
    "    Args:\n",
    "        simulation_start (datetime.datetime): The start of the simulation.\n",
    "        simulation_end (datetime.datetime): The end of the simulation.\n",
    "        learning_config (LearningConfig): The configuration for the learning process.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def load_inter_episodic_data(self, inter_episodic_data):\n",
    "        \"\"\"\n",
    "        Load the inter-episodic data from the dict stored across simulation runs.\n",
    "\n",
    "        Args:\n",
    "            inter_episodic_data (dict): The inter-episodic data to be loaded.\n",
    "\n",
    "        \"\"\"\n",
    "        self.episodes_done = inter_episodic_data[\"episodes_done\"]\n",
    "        self.eval_episodes_done = inter_episodic_data[\"eval_episodes_done\"]\n",
    "        self.max_eval = inter_episodic_data[\"max_eval\"]\n",
    "        self.rl_eval = inter_episodic_data[\"all_eval\"]\n",
    "        self.avg_rewards = inter_episodic_data[\"avg_all_eval\"]\n",
    "        self.buffer = inter_episodic_data[\"buffer\"]\n",
    "\n",
    "        # if enough initial experience was collected according to specifications in learning config\n",
    "        # turn off initial exploration and go into full learning mode\n",
    "        if self.episodes_done > self.episodes_collecting_initial_experience:\n",
    "            self.turn_off_initial_exploration()\n",
    "\n",
    "        self.set_noise_scale(inter_episodic_data[\"noise_scale\"])\n",
    "\n",
    "        self.initialize_policy(inter_episodic_data[\"actors_and_critics\"])\n",
    "\n",
    "    def get_inter_episodic_data(self):\n",
    "        \"\"\"\n",
    "        Dump the inter-episodic data to a dict for storing across simulation runs.\n",
    "\n",
    "        Returns:\n",
    "            dict: The inter-episodic data to be stored.\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"episodes_done\": self.episodes_done,\n",
    "            \"eval_episodes_done\": self.eval_episodes_done,\n",
    "            \"max_eval\": self.max_eval,\n",
    "            \"all_eval\": self.rl_eval,\n",
    "            \"avg_all_eval\": self.avg_rewards,\n",
    "            \"buffer\": self.buffer,\n",
    "            \"actors_and_critics\": self.rl_algorithm.extract_policy(),\n",
    "            \"noise_scale\": self.get_noise_scale(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics in `inter_episodic_data` are stored for the following reasons:\n",
    "\n",
    "- `episodes_done` and `eval_episodes_done`: **Monitoring Progress**  \n",
    "  Keeping track of the number of episodes completed.\n",
    "\n",
    "- `max_eval`, `all_eval`, `avg_all_eval`: **Evaluating Performance**  \n",
    "  Storing evaluation scores and average rewards to assess the agent's performance across episodes.\n",
    "\n",
    "- `buffer`: **Experience Replay**  \n",
    "  Using a replay buffer to learn from past experiences and improve data efficiency.\n",
    "\n",
    "- `noise_scale`: **Policy Exploration**  \n",
    "  The noise is used to include exploration in the policy. It is decreased across episode numbers, and we store the current noise value to continue the decrease across future episodes.\n",
    "\n",
    "- `actors_and_critics`: **Policy Initialization**  \n",
    "  Initializing the policy with actors and critics (`self.initialize_policy()`) ensures that the agent starts with the pre-defined strategy from the previous episode and can improve upon it through learning.\n",
    "\n",
    "\n",
    "### 2.2 Learning Algorithm\n",
    "\n",
    "If learning is used, then the learning role initializes a learning algorithm which is the heart of the learning progress. Currently, only the MATD3 is implemented, but we are working on different PPO implementations as well. If you would like to add an algoithm it woulb be integrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0ww-L9fABnw3"
   },
   "outputs": [],
   "source": [
    "class Learning(Learning):\n",
    "    def create_learning_algorithm(self, algorithm: RLAlgorithm):\n",
    "        \"\"\"\n",
    "        Create and initialize the reinforcement learning algorithm.\n",
    "\n",
    "        This method creates and initializes the reinforcement learning algorithm based on the specified algorithm name. The algorithm\n",
    "        is associated with the learning role and configured with relevant hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            algorithm (RLAlgorithm): The name of the reinforcement learning algorithm.\n",
    "        \"\"\"\n",
    "        if algorithm == \"matd3\":\n",
    "            self.rl_algorithm = TD3(\n",
    "                learning_role=self,\n",
    "                learning_rate=self.learning_rate,\n",
    "                episodes_collecting_initial_experience=self.episodes_collecting_initial_experience,\n",
    "                gradient_steps=self.gradient_steps,\n",
    "                batch_size=self.batch_size,\n",
    "                gamma=self.gamma,\n",
    "                actor_architecture=self.actor_architecture,\n",
    "            )\n",
    "        else:\n",
    "            logger.error(f\"Learning algorithm {algorithm} not implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Learning Algorithm Flow in Assume\n",
    "\n",
    "The following graph illustrates the structure and flow of the learning algorithm within the reinforcement learning framework.\n",
    "\n",
    "<img src=\"../../docs/source/img/TD3_algorithm.jpeg\" alt=\"Learning Algorithm Graph\" height=\"400\">\n",
    "\n",
    "Within the algorithm, we distinguish three different steps that are translated into ASSUME in the following way:\n",
    "\n",
    "1. **Initialization**: This is the first step where all necessary components such as the actors, critics, and buffer are set up.\n",
    "\n",
    "2. **Experience Collection**: The second step, represented in the flowchart above within the loop, involves the collection of experience. This includes choosing an action, observing a reward, and storing the transition tuple in the buffer.\n",
    "\n",
    "3. **Policy Update**: The third step is the actual policy update, which is also performed within the loop, allowing the agent to improve its performance over time.\n",
    "\n",
    "\n",
    "### 3.1 Initialization\n",
    "\n",
    "The initialization of the actors, critics, and the buffer is handled via the `learning_role` and the `inter_episodic_data`, as described earlier. The `create_learning_algorithm` function triggers their initialization in `initialize_policy`. At the beginning of the training process, they are initialized with new random settings. In subsequent episodes, they are initialized with pre-learned data, ensuring that previous learning is retained and built upon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def initialize_policy(self, actors_and_critics: dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Create actor and critic networks for reinforcement learning.\n",
    "\n",
    "        If `actors_and_critics` is None, this method creates new actor and critic networks.\n",
    "        If `actors_and_critics` is provided, it assigns existing networks to the respective attributes.\n",
    "\n",
    "        Args:\n",
    "            actors_and_critics (dict): The actor and critic networks to be assigned.\n",
    "\n",
    "        \"\"\"\n",
    "        if actors_and_critics is None:\n",
    "            self.create_actors()\n",
    "            self.create_critics()\n",
    "\n",
    "        else:\n",
    "            self.learning_role.critics = actors_and_critics[\"critics\"]\n",
    "            self.learning_role.target_critics = actors_and_critics[\"target_critics\"]\n",
    "            for u_id, unit_strategy in self.learning_role.rl_strats.items():\n",
    "                unit_strategy.actor = actors_and_critics[\"actors\"][u_id]\n",
    "                unit_strategy.actor_target = actors_and_critics[\"actor_targets\"][u_id]\n",
    "\n",
    "            self.obs_dim = actors_and_critics[\"obs_dim\"]\n",
    "            self.act_dim = actors_and_critics[\"act_dim\"]\n",
    "            self.unique_obs_dim = actors_and_critics[\"unique_obs_dim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please also note that we make a distinction in the handling of the critics and target critics compared to the actors and target actors. You can observe this in the `initialize_policy` function. For the critics, they are assigned to the `learning_role` as there are centralized critics used for all the different actors. In contrast, the actors are assigned to specific unit strategies. Each learning unit, such as a power plant, has one learning strategy and therefore an individual actor, while the critics remain centralized.\n",
    "\n",
    "This distinction leads to the case where, even if learning is not active, we still need the actors to perform the entire simulation using pre-trained policies. This is essential, for example, when running simulations with previously learned policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Experience Collection\n",
    "\n",
    "Within the loop, the selection of an action with exploration noise, as well as the observation of a new reward and state, and the storing of this tuple in the buffer, are all handled within the bidding strategy. \n",
    "\n",
    "This specific process is covered in more detail in another tutorial. For more details, refer to [tutorial 04](04_reinforcement_learning_example.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Policy Update \n",
    "\n",
    "The core of the algorithm, which comprises all other steps is embodied by the `assume.reinforcement_learning.algorithms.matd3.TD3.update_policy` function in the learning algorithms. Here, the critic and the actor are updated according to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def update_policy(self):\n",
    "        \"\"\"\n",
    "        Update the policy of the reinforcement learning agent using the Twin Delayed Deep Deterministic Policy Gradients (TD3) algorithm.\n",
    "\n",
    "        Notes:\n",
    "            This function performs the policy update step, which involves updating the actor (policy) and critic (Q-function) networks\n",
    "            using TD3 algorithm. It iterates over the specified number of gradient steps and performs the following steps for each\n",
    "            learning strategy:\n",
    "\n",
    "            1. Sample a batch of transitions from the replay buffer.\n",
    "            2. Calculate the next actions with added noise using the actor target network.\n",
    "            3. Compute the target Q-values based on the next states, rewards, and the target critic network.\n",
    "            4. Compute the critic loss as the mean squared error between current Q-values and target Q-values.\n",
    "            5. Optimize the critic network by performing a gradient descent step.\n",
    "            6. Optionally, update the actor network if the specified policy delay is reached.\n",
    "            7. Apply Polyak averaging to update target networks.\n",
    "\n",
    "            This function implements the TD3 algorithm's key step for policy improvement and exploration.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.debug(\"Updating Policy\")\n",
    "        n_rl_agents = len(self.learning_role.rl_strats.keys())\n",
    "        for _ in range(self.gradient_steps):\n",
    "            self.n_updates += 1\n",
    "            i = 0\n",
    "\n",
    "            for u_id in self.learning_role.rl_strats.keys():\n",
    "                critic_target = self.learning_role.target_critics[u_id]\n",
    "                critic = self.learning_role.critics[u_id]\n",
    "                actor = self.learning_role.rl_strats[u_id].actor\n",
    "                actor_target = self.learning_role.rl_strats[u_id].actor_target\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    # only update target netwroks every 100 steps, to have delayed network update\n",
    "                    transitions = self.learning_role.buffer.sample(self.batch_size)\n",
    "                    states = transitions.observations\n",
    "                    actions = transitions.actions\n",
    "                    next_states = transitions.next_observations\n",
    "                    rewards = transitions.rewards\n",
    "\n",
    "                    with th.no_grad():\n",
    "                        # Select action according to policy and add clipped noise\n",
    "                        noise = actions.clone().data.normal_(\n",
    "                            0, self.target_policy_noise\n",
    "                        )\n",
    "                        noise = noise.clamp(\n",
    "                            -self.target_noise_clip, self.target_noise_clip\n",
    "                        )\n",
    "                        next_actions = [\n",
    "                            (actor_target(next_states[:, i, :]) + noise[:, i, :]).clamp(\n",
    "                                -1, 1\n",
    "                            )\n",
    "                            for i in range(n_rl_agents)\n",
    "                        ]\n",
    "                        next_actions = th.stack(next_actions)\n",
    "\n",
    "                        next_actions = next_actions.transpose(0, 1).contiguous()\n",
    "                        next_actions = next_actions.view(-1, n_rl_agents * self.act_dim)\n",
    "\n",
    "                all_actions = actions.view(self.batch_size, -1)\n",
    "\n",
    "                # this takes the unique observations from all other agents assuming that\n",
    "                # the unique observations are at the end of the observation vector\n",
    "                temp = th.cat(\n",
    "                    (\n",
    "                        states[:, :i, self.obs_dim - self.unique_obs_dim :].reshape(\n",
    "                            self.batch_size, -1\n",
    "                        ),\n",
    "                        states[\n",
    "                            :, i + 1 :, self.obs_dim - self.unique_obs_dim :\n",
    "                        ].reshape(self.batch_size, -1),\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "\n",
    "                # the final all_states vector now contains the current agent's observation\n",
    "                # and the unique observations from all other agents\n",
    "                all_states = th.cat(\n",
    "                    (states[:, i, :].reshape(self.batch_size, -1), temp), axis=1\n",
    "                ).view(self.batch_size, -1)\n",
    "                # all_states = states[:, i, :].reshape(self.batch_size, -1)\n",
    "\n",
    "                # this is the same as above but for the next states\n",
    "                temp = th.cat(\n",
    "                    (\n",
    "                        next_states[\n",
    "                            :, :i, self.obs_dim - self.unique_obs_dim :\n",
    "                        ].reshape(self.batch_size, -1),\n",
    "                        next_states[\n",
    "                            :, i + 1 :, self.obs_dim - self.unique_obs_dim :\n",
    "                        ].reshape(self.batch_size, -1),\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "\n",
    "                # the final all_next_states vector now contains the current agent's observation\n",
    "                # and the unique observations from all other agents\n",
    "                all_next_states = th.cat(\n",
    "                    (next_states[:, i, :].reshape(self.batch_size, -1), temp), axis=1\n",
    "                ).view(self.batch_size, -1)\n",
    "                # all_next_states = next_states[:, i, :].reshape(self.batch_size, -1)\n",
    "\n",
    "                with th.no_grad():\n",
    "                    # Compute the next Q-values: min over all critics targets\n",
    "                    next_q_values = th.cat(\n",
    "                        critic_target(all_next_states, next_actions), dim=1\n",
    "                    )\n",
    "                    next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n",
    "                    target_Q_values = (\n",
    "                        rewards[:, i].unsqueeze(1) + self.gamma * next_q_values\n",
    "                    )\n",
    "\n",
    "                # Get current Q-values estimates for each critic network\n",
    "                current_Q_values = critic(all_states, all_actions)\n",
    "\n",
    "                # Compute critic loss\n",
    "                critic_loss = sum(\n",
    "                    F.mse_loss(current_q, target_Q_values)\n",
    "                    for current_q in current_Q_values\n",
    "                )\n",
    "\n",
    "                # Optimize the critics\n",
    "                critic.optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                critic.optimizer.step()\n",
    "\n",
    "                # Delayed policy updates\n",
    "                if self.n_updates % self.policy_delay == 0:\n",
    "                    # Compute actor loss\n",
    "                    state_i = states[:, i, :]\n",
    "                    action_i = actor(state_i)\n",
    "\n",
    "                    all_actions_clone = actions.clone()\n",
    "                    all_actions_clone[:, i, :] = action_i\n",
    "                    all_actions_clone = all_actions_clone.view(self.batch_size, -1)\n",
    "\n",
    "                    actor_loss = -critic.q1_forward(\n",
    "                        all_states, all_actions_clone\n",
    "                    ).mean()\n",
    "\n",
    "                    actor.optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor.optimizer.step()\n",
    "\n",
    "                    polyak_update(\n",
    "                        critic.parameters(), critic_target.parameters(), self.tau\n",
    "                    )\n",
    "                    polyak_update(\n",
    "                        actor.parameters(), actor_target.parameters(), self.tau\n",
    "                    )\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other functions within the reinforcement learning algorithm are primarily there to store, update, and save the new policies. These functions either write the updated policies to a designated location or save them into the `inter_episodic_data`.\n",
    "\n",
    "If you would like to make a change to this algorithm, the most likely modification would be to the `update_policy` function, as it plays a central role in the learning process. The other functions would only need adjustments if the different algorithm features vary likethe target critics or critic architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3flH5iY4x7Z"
   },
   "source": [
    "### 3.5 Start the simulation\n",
    "\n",
    "We are almost done with all the changes to actually be able to make ASSUME learn here in google colab. If you would rather like to load our pretrained strategies, we need a function for loading parameters, which can be found below.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTlqMouufKyo"
   },
   "source": [
    "To control the learning process, the config file determines the parameters of the learning algorithm. As we want to temper with these values in the notebook we will overwrite the learning config in the next cell and then load it into our world.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "moZ_UD7FfkOh"
   },
   "outputs": [],
   "source": [
    "learning_config = {\n",
    "    \"continue_learning\": False,\n",
    "    \"trained_policies_save_path\": \"None\",\n",
    "    \"max_bid_price\": 100,\n",
    "    \"algorithm\": \"matd3\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_episodes\": 100,\n",
    "    \"episodes_collecting_initial_experience\": 5,\n",
    "    \"train_freq\": \"24h\",\n",
    "    \"gradient_steps\": -1,\n",
    "    \"batch_size\": 256,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"noise_sigma\": 0.1,\n",
    "    \"noise_scale\": 1,\n",
    "    \"noise_dt\": 1,\n",
    "    \"validation_episodes_interval\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "iPz8v4N5hpfr"
   },
   "outputs": [],
   "source": [
    "# Read the YAML file\n",
    "with open(\"assume/examples/inputs/example_02a/config.yaml\") as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "# store our modifications to the config file\n",
    "data[\"base\"][\"learning_mode\"] = True\n",
    "data[\"base\"][\"learning_config\"] = learning_config\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open(\"assume/examples/inputs/example_02a/config.yaml\", \"w\") as file:\n",
    "    yaml.safe_dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlRnTgCy5d9W"
   },
   "source": [
    "In order to let the simulation run with the integrated learning we need to touch up the main file that runs it in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZlWxXxZr54WV",
    "outputId": "e30f4279-7a4e-4efc-9cfb-61416e4fe2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.world:connected to db\n",
      "INFO:assume.scenario.loader_csv:Starting Scenario example_02a/base from assume/examples/inputs\n",
      "INFO:assume.scenario.loader_csv:storage_units not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:industrial_dsm_units not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:forecasts_df not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:Downsampling demand_df successful.\n",
      "INFO:assume.scenario.loader_csv:cross_border_flows not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:availability_df not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:electricity_prices not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:price_forecasts not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:temperature not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n",
      "INFO:assume.scenario.loader_csv:storage_units not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:industrial_dsm_units not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:forecasts_df not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:Downsampling demand_df successful.\n",
      "INFO:assume.scenario.loader_csv:cross_border_flows not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:availability_df not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:electricity_prices not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:price_forecasts not found. Returning None\n",
      "INFO:assume.scenario.loader_csv:temperature not found. Returning None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "example_02a_base_1 2019-03-31 00:00:00: : 2592001.0it [00:27, 94420.77it/s]                           \n",
      "Training Episodes:   1%|          | 1/100 [00:27<45:21, 27.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "example_02a_base_2 2019-03-31 00:00:00: : 2592001.0it [00:27, 94242.68it/s]                           \n",
      "Training Episodes:   2%|▏         | 2/100 [00:55<44:56, 27.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "example_02a_base_3 2019-03-31 00:00:00: : 2592001.0it [00:27, 93778.01it/s]\n",
      "Training Episodes:   3%|▎         | 3/100 [01:22<44:36, 27.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "example_02a_base_4 2019-03-31 00:00:00: : 2592001.0it [00:27, 93996.50it/s]                           \n",
      "Training Episodes:   4%|▍         | 4/100 [01:50<44:09, 27.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:   5%|▌         | 5/100 [02:16<42:57, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:assume.scenario.loader_csv:Adding markets\n",
      "INFO:assume.scenario.loader_csv:Read units from file\n",
      "INFO:assume.scenario.loader_csv:Adding power_plant units\n",
      "INFO:assume.scenario.loader_csv:Adding demand units\n",
      "INFO:assume.scenario.loader_csv:Adding unit operators and units\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Episodes:   5%|▌         | 5/100 [02:16<43:20, 27.37s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Time must be > 1553886000.0 but is 1551398399.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# run learning if learning mode is enabled\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# needed as we simulate the modelling horizon multiple times to train reinforcement learning run_learning( world, inputs_path=input_path, scenario=scenario, study_case=study_case, )\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world\u001b[38;5;241m.\u001b[39mlearning_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 47\u001b[0m     \u001b[43mrun_learning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy_case\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudy_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# after the learning is done we make a normal run of the simulation, which equasl a test run\u001b[39;00m\n\u001b[0;32m     55\u001b[0m world\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[1;32mIn[17], line 102\u001b[0m, in \u001b[0;36mrun_learning\u001b[1;34m(world, inputs_path, scenario, study_case, verbose)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Give the newly initliazed learning role the needed information across episodes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m world\u001b[38;5;241m.\u001b[39mlearning_role\u001b[38;5;241m.\u001b[39mload_inter_episodic_data(inter_episodic_data)\n\u001b[1;32m--> 102\u001b[0m \u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Store updated information across episodes\u001b[39;00m\n\u001b[0;32m    105\u001b[0m inter_episodic_data \u001b[38;5;241m=\u001b[39m world\u001b[38;5;241m.\u001b[39mlearning_role\u001b[38;5;241m.\u001b[39mget_inter_episodic_data()\n",
      "File \u001b[1;32m~\\Documents\\Code\\assume\\assume\\world.py:686\u001b[0m, in \u001b[0;36mWorld.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m end_ts \u001b[38;5;241m=\u001b[39m datetime2timestamp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend)\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masync_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_ts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_ts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_ts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tg3533\\AppData\\Local\\miniconda3\\envs\\assume-framework\\Lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tg3533\\AppData\\Local\\miniconda3\\envs\\assume-framework\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\tg3533\\AppData\\Local\\miniconda3\\envs\\assume-framework\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32m~\\Documents\\Code\\assume\\assume\\world.py:651\u001b[0m, in \u001b[0;36mWorld.async_run\u001b[1;34m(self, start_ts, end_ts)\u001b[0m\n\u001b[0;32m    648\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mend_ts \u001b[38;5;241m-\u001b[39m start_ts)\n\u001b[0;32m    650\u001b[0m \u001b[38;5;66;03m# allow registration before first opening\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_ts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_role \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock_manager\u001b[38;5;241m.\u001b[39mbroadcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtime)\n",
      "File \u001b[1;32mc:\\Users\\tg3533\\AppData\\Local\\miniconda3\\envs\\assume-framework\\Lib\\site-packages\\mango\\util\\clock.py:70\u001b[0m, in \u001b[0;36mExternalClock.set_time\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03mNew time is set\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime must be > \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# set time\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time \u001b[38;5;241m=\u001b[39m t\n",
      "\u001b[1;31mValueError\u001b[0m: Time must be > 1553886000.0 but is 1551398399."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:assume.markets.base_market:1553889600.0 Market result [(datetime.datetime(2019, 3, 29, 20, 0), datetime.datetime(2019, 3, 29, 21, 0), None)] for market EOM are empty!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from assume.strategies.learning_strategies import RLStrategy\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "csv_path = \"./outputs\"\n",
    "os.makedirs(\"./local_db\", exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Available examples:\n",
    "    - local_db: without database and grafana\n",
    "    - timescale: with database and grafana (note: you need docker installed)\n",
    "    \"\"\"\n",
    "    data_format = \"local_db\"  # \"local_db\" or \"timescale\"\n",
    "\n",
    "    if data_format == \"local_db\":\n",
    "        db_uri = \"sqlite:///./local_db/assume_db.db\"\n",
    "    elif data_format == \"timescale\":\n",
    "        db_uri = \"postgresql://assume:assume@localhost:5432/assume\"\n",
    "\n",
    "    input_path = \"assume/examples/inputs\"\n",
    "    scenario = \"example_02a\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # create world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # we import our defined bidding strategey class including the learning into the world bidding strategies\n",
    "    # in the example files we provided the name of the learning bidding strategeis in the input csv is  \"pp_learning\"\n",
    "    # hence we define this strategey to be one of the learning class\n",
    "    world.bidding_strategies[\"pp_learning\"] = RLStrategy\n",
    "\n",
    "    # then we load the scenario specified above from the respective input files\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=input_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # run learning if learning mode is enabled\n",
    "    # needed as we simulate the modelling horizon multiple times to train reinforcement learning run_learning( world, inputs_path=input_path, scenario=scenario, study_case=study_case, )\n",
    "\n",
    "    if world.learning_config.get(\"learning_mode\", False):\n",
    "        run_learning(\n",
    "            world,\n",
    "            inputs_path=input_path,\n",
    "            scenario=scenario,\n",
    "            study_case=study_case,\n",
    "        )\n",
    "\n",
    "    # after the learning is done we make a normal run of the simulation, which equasl a test run\n",
    "    world.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
